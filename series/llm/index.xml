<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on 多辣加香菜</title>
    <link>http://xilyfeAAAA.github.io/series/llm/</link>
    <description>Recent content in LLM on 多辣加香菜</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 24 Feb 2026 12:35:13 +0800</lastBuildDate>
    <atom:link href="http://xilyfeAAAA.github.io/series/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>transformers 库提供的更方便的 Trainer</title>
      <link>http://xilyfeAAAA.github.io/posts/trainer/</link>
      <pubDate>Wed, 18 Feb 2026 21:49:24 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/trainer/</guid>
      <description>&lt;h2 id=&#34;sfttrainer&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#sfttrainer&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;SFTTrainer&lt;/h2&gt;&lt;h3 id=&#34;函数调用&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%87%bd%e6%95%b0%e8%b0%83%e7%94%a8&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;函数调用&lt;/h3&gt;&lt;p&gt;我们主要依赖 Huggingface 的 transformers 库以及 trl 库来进行自动化微调。&lt;/p&gt;&#xA;&lt;div class=&#34;code-block highlight is-open show-line-numbers  tw-group tw-my-2&#34;&gt;&#xA;  &lt;div class=&#34;&#xA;    &#xA;    tw-flex &#xA;    tw-flex-row&#xA;    tw-flex-1 &#xA;    tw-justify-between &#xA;    tw-w-full tw-bg-bgColor-secondary&#xA;    &#34;&gt;      &#xA;    &lt;button &#xA;      class=&#34;&#xA;        code-block-button&#xA;        tw-mx-2 &#xA;        tw-flex&#xA;        tw-flex-row&#xA;        tw-flex-1&#34;&#xA;      aria-hidden=&#34;true&#34;&gt;&#xA;          &lt;div class=&#34;group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 320 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z&#34;/&gt;&lt;/svg&gt;&lt;/div&gt;&#xA;          &lt;p class=&#34;tw-select-none !tw-my-1&#34;&gt;python&lt;/p&gt;</description>
    </item>
    <item>
      <title>NF4 量化大模型</title>
      <link>http://xilyfeAAAA.github.io/posts/nf4-quantization/</link>
      <pubDate>Tue, 17 Feb 2026 14:11:42 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/nf4-quantization/</guid>
      <description>&lt;h2 id=&#34;nf4-量化&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#nf4-%e9%87%8f%e5%8c%96&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;NF4 量化&lt;/h2&gt;&lt;p&gt;介绍 NF4 量化之前我们先说说量化是什么，量化本质是把 &lt;strong&gt;浮点数张量压缩到有限的整数集合里&lt;/strong&gt;。例如 INT4 量化，就是把所有浮点数映射到 $2^4$ 个整数也就是 [-8, 7] 的区间内，这样子就只需要存储这 4个 bit 也就是 0.5B。具体的公式是：&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoRA&amp;QLoRA</title>
      <link>http://xilyfeAAAA.github.io/posts/loraqlora/</link>
      <pubDate>Mon, 16 Feb 2026 20:21:20 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/loraqlora/</guid>
      <description>&lt;h2 id=&#34;lora&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#lora&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;LoRA&lt;/h2&gt;&lt;h3 id=&#34;为什么选择旁路而不是堆叠&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9%e6%97%81%e8%b7%af%e8%80%8c%e4%b8%8d%e6%98%af%e5%a0%86%e5%8f%a0&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;为什么选择旁路而不是堆叠&lt;/h3&gt;&lt;p&gt;LoRA 现在的方案是 $y=Wx+\Delta{W}x$ 这样在旁路加一个矩阵进行微调，那为什么不选择 $y=W_2(W_1x)$ 这样的堆叠方案呢？&lt;/p&gt;&#xA;&lt;p&gt;首先这种旁路设计可以保持原权重函数不变。LoRA 只提供一个低秩修正项。初始化时 A、B 接近零，模型行为≈原模型。训练是“微调偏移量”，不是“重建映射”。如果改成堆叠新层，前向函数直接改变，初始输出就漂移，大模型容易不稳定。&lt;/p&gt;</description>
    </item>
    <item>
      <title>模型的 generate 方法</title>
      <link>http://xilyfeAAAA.github.io/posts/llm-generate/</link>
      <pubDate>Wed, 11 Feb 2026 11:19:33 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/llm-generate/</guid>
      <description>&lt;p&gt;在 MiniMind 系列的 eval 部分我们已经学习了如何通过 transformers 库里 GenerateMixin 基类来生成文本，这一章学习一下 &lt;code&gt;model.generate()&lt;/code&gt; 方法到底是怎么实现的。&lt;/p&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;post-preview&#34;&gt;&#xA;  &lt;div class=&#34;post-preview--meta&#34; style=&#34;width:100%;&#34;&gt;&#xA;    &lt;div class=&#34;post-preview--middle&#34;&gt;&#xA;      &lt;h4 class=&#34;post-preview--title&#34;&gt;&#xA;        &lt;a target=&#34;_blank&#34; href=&#34;http://xilyfeAAAA.github.io/posts/minimind-eval/&#34;&gt;MiniMind 学习指北(四)：评估&lt;/a&gt;&#xA;      &lt;/h4&gt;&#xA;      &lt;time class=&#34;post-preview--date&#34;&gt;2026-01-25&lt;/time&gt;&#xA;      &#xA;      &lt;small&gt;#大模型&amp;nbsp;#深度学习&amp;nbsp;&lt;/small&gt;&#xA;      &#xA;      &lt;section style=&#34;max-height:105px;overflow:hidden;&#34; class=&#34;post-preview--excerpt&#34;&gt;&#xA;         这一章我们需要设计一个脚本来验证大模型的对话能力&#xA;评估脚本我们预训练是让模型学会说话的能力，或者说词语接龙的能力，给他一个 prompt 它可以接着说下去。因此我们在处理 prompt 时候需要稍加处理：&#xA;python&#xA;&#xA;      &lt;/section&gt;&#xA;    &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;为什么需要-generate&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-generate&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;为什么需要 Generate？&lt;/h2&gt;&lt;p&gt;Transformer 模型在训练时有一个 &lt;code&gt;forward&lt;/code&gt; 方法，是用于针对模型的输入来产生输出，从而计算损失 loss，更新模型的参数。既然有这么一个生成的函数了，为什么 Transformer 中还有专门设计 &lt;code&gt;generate&lt;/code&gt; 方法来负责在推理时生成文本呢？&lt;/p&gt;</description>
    </item>
    <item>
      <title>MoE 混合专家模型</title>
      <link>http://xilyfeAAAA.github.io/posts/moe/</link>
      <pubDate>Fri, 06 Feb 2026 11:53:20 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/moe/</guid>
      <description>&lt;h2 id=&#34;moe-是什么&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#moe-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;MoE 是什么&lt;/h2&gt;&lt;p&gt;MoE means Mixture of Experts，它是一种神经网络架构，可以把一个大模型拆分成多个小型的 expert，再用一个门控网络来决定每个输入该路由到哪些专家处理。&lt;/p&gt;</description>
    </item>
    <item>
      <title>分布式训练技术</title>
      <link>http://xilyfeAAAA.github.io/posts/distributed_parallel/</link>
      <pubDate>Mon, 02 Feb 2026 19:50:43 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/distributed_parallel/</guid>
      <description>&lt;p&gt;我们先回忆一下传统的单机单卡训练模式：&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&lt;div class=&#34;post-img-view&#34;&gt;&#xA;&lt;a data-fancybox=&#34;gallery&#34; href=&#34;http://img.xilyfe.top/img/20260202195324965.png&#34;&gt;&#xA;&lt;img src=&#34;http://img.xilyfe.top/img/20260202195324965.png&#34; alt=&#34;&#34;  /&gt;&#xA;&lt;/a&gt;&#xA;&lt;/div&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;首先硬盘读取数据，CPU 处理数据，将数据组成一个 batch，再传入 GPU，网络前向传播算出 loss，再反向传播计算梯度，用梯度更新参数完成一次训练。这种传统模式在大参数量或者大数据量的情况下就容易陷入显存的瓶颈，于是就引出了多卡并行训练。&lt;/p&gt;</description>
    </item>
    <item>
      <title>transformer库的基类</title>
      <link>http://xilyfeAAAA.github.io/posts/hg_transformer/</link>
      <pubDate>Tue, 27 Jan 2026 19:16:19 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/hg_transformer/</guid>
      <description>&lt;h2 id=&#34;长话短说&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e9%95%bf%e8%af%9d%e7%9f%ad%e8%af%b4&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;长话短说&lt;/h2&gt;&lt;p&gt;今天看了一下源码和官方文档，梳理过后会发现其实也不是很复杂，简单理解就两条：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;ModelOutput&lt;/code&gt;(transformers.utils.ModelOutput)是所有模型输出的基类。简单理解它就是一个字典，在模型的 &lt;code&gt;forward&lt;/code&gt;函数里把原本的输出做了一下封装而已，方便用户能直观地知道输出是什么。例如&lt;code&gt;CausalLMOutput&lt;/code&gt;顾名思义就是用于像 GPT 这样自回归模型的输出。&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;PreTrainedModel&lt;/code&gt; (transformers.modeling_utils.PretrainedModel) 是所有模型的基类。所以你如果看到一个模型取名为&lt;code&gt;LlamaForCausalLM&lt;/code&gt;，那你就可以知道这个模型的输出格式大概率就是自回归输出，即前面提到的&lt;code&gt;CausalLMOutput&lt;/code&gt;。为什么说大概率呢，因为自回归输出还有蛮多种的，赶时间的朋友看到这就可以切换到其他文章了，至此你应该也能了解 transformers 最核心的模块了。感兴趣的可以继续往下看，下面做一个简单的总结和介绍。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;短话长说&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e7%9f%ad%e8%af%9d%e9%95%bf%e8%af%b4&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;短话长说&lt;/h2&gt;&lt;h3 id=&#34;modeloutput&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#modeloutput&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;ModelOutput&lt;/h3&gt;&lt;p&gt;前面已经介绍过了，&lt;code&gt;ModelOutput&lt;/code&gt;是所有模型输出的基类。下面是其源码核心部分，一些具体实现代码删除了，不过不影响理解。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KVCache</title>
      <link>http://xilyfeAAAA.github.io/posts/kvcache/</link>
      <pubDate>Wed, 03 Dec 2025 16:25:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/kvcache/</guid>
      <description>&lt;div class=&#34;details admonition tip open&#34;&gt;&#xA;    &lt;div class=&#34;details-summary admonition-title&#34;&gt;&#xA;        &lt;span class=&#34;icon&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 352 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M96.06 454.35c.01 6.29 1.87 12.45 5.36 17.69l17.09 25.69a31.99 31.99 0 0 0 26.64 14.28h61.71a31.99 31.99 0 0 0 26.64-14.28l17.09-25.69a31.989 31.989 0 0 0 5.36-17.69l.04-38.35H96.01l.05 38.35zM0 176c0 44.37 16.45 84.85 43.56 115.78 16.52 18.85 42.36 58.23 52.21 91.45.04.26.07.52.11.78h160.24c.04-.26.07-.51.11-.78 9.85-33.22 35.69-72.6 52.21-91.45C335.55 260.85 352 220.37 352 176 352 78.61 272.91-.3 175.45 0 73.44.31 0 82.97 0 176zm176-80c-44.11 0-80 35.89-80 80 0 8.84-7.16 16-16 16s-16-7.16-16-16c0-61.76 50.24-112 112-112 8.84 0 16 7.16 16 16s-7.16 16-16 16z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;KVCache 的应用场景&lt;span class=&#34;details-icon&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 256 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;details-content&#34;&gt;&#xA;        &lt;div class=&#34;admonition-content&#34;&gt;KV Cache 的主要目的是 “加速自回归生成”，在生成下一个 token 时，只需要计算当前 token 的 query、key、value。之前的 key/value 被缓存，直接拼接使用，避免重复计算整个历史序列的 attention，这在推理阶段能显著提升速度。但是在预训练阶段，例如 LLaMa 或者 GPT 采用的都是 Teacher-Forcing 这种并行训练，我们把 logits 和 labels 错位。这种计算是并行的，不需要“增量”生成，所以使用 KV Cache 反而会增加代码复杂度和内存开销，收益很小甚至为负。&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#xA;&lt;h2 id=&#34;前情提要&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%89%8d%e6%83%85%e6%8f%90%e8%a6%81&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Teacher-Forcing vs 自回归生成&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RoPE</title>
      <link>http://xilyfeAAAA.github.io/posts/rope/</link>
      <pubDate>Fri, 28 Nov 2025 16:23:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/rope/</guid>
      <description>&lt;h2 id=&#34;作用&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e4%bd%9c%e7%94%a8&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;作用&lt;/h2&gt;&lt;p&gt;RoPE 相对于正余弦位置编码和可学习位置编码，更能够表达&lt;strong&gt;相对位置信息&lt;/strong&gt;，便于模型捕捉序列中元素之间的关系，还便于模型泛化到更长的序列，支持超长文本推理。&lt;/p&gt;</description>
    </item>
    <item>
      <title>估算模型需要的显存</title>
      <link>http://xilyfeAAAA.github.io/posts/llm_vram/</link>
      <pubDate>Fri, 22 Aug 2025 16:56:43 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/llm_vram/</guid>
      <description>&lt;p&gt;到底需要多少算力才能部署一个模型，这是一个非常常见的问题。我们就从训练和推理两个场景，分析一下如何估计模型所需要的显存。&lt;/p&gt;&#xA;&lt;h2 id=&#34;训练&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e8%ae%ad%e7%bb%83&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;训练&lt;/h2&gt;&lt;p&gt;训练显存大致分为以下四部分：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;模型权重：取决于存储的精度，常见的 BF16 和 FP16 占用大小为 2B&lt;/li&gt;&#xA;&lt;li&gt;梯度：反向传播计算的梯度，和权重一样常见情况下占用 2B&lt;/li&gt;&#xA;&lt;li&gt;优化器状态：常见的 Adam 会为每个参数都保存它的 Momentum、Variance 和 Master weights，精度为 FP32 所以总计 12B&lt;/li&gt;&#xA;&lt;li&gt;中间激活值：简单来说就是为了计算反向传播的梯度，需要把前向计算的中间值存储起来，具体计算见下文。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;因此使用 AdamW 优化器 + 混合精度训练的经验公式为：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
