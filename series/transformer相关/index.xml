<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer相关 on 多辣加香菜</title>
    <link>http://xilyfeAAAA.github.io/series/transformer%E7%9B%B8%E5%85%B3/</link>
    <description>Recent content in Transformer相关 on 多辣加香菜</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 18 Feb 2026 09:51:55 +0800</lastBuildDate>
    <atom:link href="http://xilyfeAAAA.github.io/series/transformer%E7%9B%B8%E5%85%B3/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>transformers 库提供的更方便的 Trainer</title>
      <link>http://xilyfeAAAA.github.io/posts/trainer/</link>
      <pubDate>Wed, 18 Feb 2026 21:49:24 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/trainer/</guid>
      <description>&lt;h2 id=&#34;sfttrainer&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#sfttrainer&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;SFTTrainer&lt;/h2&gt;&lt;h3 id=&#34;函数调用&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%87%bd%e6%95%b0%e8%b0%83%e7%94%a8&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;函数调用&lt;/h3&gt;&lt;p&gt;我们主要依赖 Huggingface 的 transformers 库以及 trl 库来进行自动化微调。&lt;/p&gt;&#xA;&lt;div class=&#34;code-block highlight is-open show-line-numbers  tw-group tw-my-2&#34;&gt;&#xA;  &lt;div class=&#34;&#xA;    &#xA;    tw-flex &#xA;    tw-flex-row&#xA;    tw-flex-1 &#xA;    tw-justify-between &#xA;    tw-w-full tw-bg-bgColor-secondary&#xA;    &#34;&gt;      &#xA;    &lt;button &#xA;      class=&#34;&#xA;        code-block-button&#xA;        tw-mx-2 &#xA;        tw-flex&#xA;        tw-flex-row&#xA;        tw-flex-1&#34;&#xA;      aria-hidden=&#34;true&#34;&gt;&#xA;          &lt;div class=&#34;group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 320 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z&#34;/&gt;&lt;/svg&gt;&lt;/div&gt;&#xA;          &lt;p class=&#34;tw-select-none !tw-my-1&#34;&gt;python&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mask On Transformer</title>
      <link>http://xilyfeAAAA.github.io/posts/tips_about_mask/</link>
      <pubDate>Sat, 24 Jan 2026 12:22:15 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/tips_about_mask/</guid>
      <description>&lt;p&gt;在 Transformer 中同时运用了两种掩码技术：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;用于&lt;strong&gt;处理非定长序列&lt;/strong&gt;的 padding mask&lt;/li&gt;&#xA;&lt;li&gt;用于&lt;strong&gt;防止标签泄露&lt;/strong&gt;的 causal mask&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;padding-mask&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#padding-mask&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;Padding Mask&lt;/h2&gt;&lt;p&gt;NLP 任务中，输入的长度往往不是统一的，训练的数据集里面样本长度各有不同。但是我们在实际训练中，往往需要把多个数据合成一个大的 batch 一同训练，这样可以充分利用显卡的性能。那么问题就来了，不同长度的文本如何合成一个大 batch 呢。NLP 的解决思路是：把所有输入的文本统一成一个固定长度，多余的位置用特殊字符 &amp;lt;PAD&amp;gt; 来填充。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
