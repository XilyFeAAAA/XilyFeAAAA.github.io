<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Minimind on 多辣加香菜</title>
    <link>http://xilyfeAAAA.github.io/series/minimind/</link>
    <description>Recent content in Minimind on 多辣加香菜</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 19 Feb 2026 12:25:36 +0800</lastBuildDate>
    <atom:link href="http://xilyfeAAAA.github.io/series/minimind/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MiniMind 学习指北(六)：LoRA</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-lora/</link>
      <pubDate>Fri, 13 Feb 2026 14:08:48 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-lora/</guid>
      <description>&lt;h2 id=&#34;lora-是什么&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#lora-%e6%98%af%e4%bb%80%e4%b9%88&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;LoRA 是什么&lt;/h2&gt;&lt;p&gt;PEFT 大致包含三类：Prompt-Tuning、Adapter-Tuning 以及 LoRA，而 MiniMind 里面采用的就是 LoRA 进行指令微调。在 CS224N 的课程中已经学习了 LoRA 的原理，简单来说我们在经过 Pretrain 和 SFT 的模型基础上，对参数 $y=Wx$ 加上一个增量矩阵 $\Delta{W}$ 来微调模型，并且这个 $\Delta{W}$ 是通过 &lt;strong&gt;低秩近似&lt;/strong&gt; 得到的，所以实际参数量远小于 $W$，计算开销小。具体可以看之前的笔记：&lt;/p&gt;</description>
    </item>
    <item>
      <title>MiniMind 学习指北(五)：SFT</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-sft/</link>
      <pubDate>Tue, 10 Feb 2026 15:59:28 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-sft/</guid>
      <description>&lt;p&gt;前面我们已经进行了预训练，得到了一个只会续写的模型。这是因为我们预训练数据集的文本都是简单的一句话，然后通过加工我们得到了类似 &amp;ldquo;&amp;lt;|im_start|&amp;gt; 秦始皇的功绩包括统一货币、文字&amp;rdquo; 的文本，通过 Teacher-Forcing 它只能做到预测下一个 token，或者说只会机械接龙，不会对话。&lt;/p&gt;</description>
    </item>
    <item>
      <title>MiniMind 学习指北(四)：评估</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-eval/</link>
      <pubDate>Sun, 25 Jan 2026 13:47:19 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-eval/</guid>
      <description>&lt;blockquote&gt;&#xA;  &lt;p&gt;这一章我们需要设计一个脚本来验证大模型的对话能力&lt;/p&gt;&#xA;&#xA;&lt;/blockquote&gt;&lt;h2 id=&#34;评估脚本&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e8%af%84%e4%bc%b0%e8%84%9a%e6%9c%ac&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;评估脚本&lt;/h2&gt;&lt;p&gt;我们预训练是让模型学会说话的能力，或者说词语接龙的能力，给他一个 prompt 它可以接着说下去。因此我们在处理 prompt 时候需要稍加处理：&lt;/p&gt;&#xA;&lt;div class=&#34;code-block highlight is-open show-line-numbers  tw-group tw-my-2&#34;&gt;&#xA;  &lt;div class=&#34;&#xA;    &#xA;    tw-flex &#xA;    tw-flex-row&#xA;    tw-flex-1 &#xA;    tw-justify-between &#xA;    tw-w-full tw-bg-bgColor-secondary&#xA;    &#34;&gt;      &#xA;    &lt;button &#xA;      class=&#34;&#xA;        code-block-button&#xA;        tw-mx-2 &#xA;        tw-flex&#xA;        tw-flex-row&#xA;        tw-flex-1&#34;&#xA;      aria-hidden=&#34;true&#34;&gt;&#xA;          &lt;div class=&#34;group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 320 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z&#34;/&gt;&lt;/svg&gt;&lt;/div&gt;&#xA;          &lt;p class=&#34;tw-select-none !tw-my-1&#34;&gt;python&lt;/p&gt;</description>
    </item>
    <item>
      <title>MiniMind 学习指北(三)：预训练</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-pretrain/</link>
      <pubDate>Sat, 24 Jan 2026 13:47:19 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-pretrain/</guid>
      <description>&lt;h2 id=&#34;数据集&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;数据集&lt;/h2&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;预训练我们采用的是 Teacher-Forcing，所以需要的数据格式应该是偏移的 &lt;code&gt;input_ids&lt;/code&gt; 和 &lt;code&gt;labels&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;__getitem__&lt;/code&gt; 方法有几个需要注意的地方：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在 input_ids 前后加上 bos 和 eos 两个 special token 可以帮助模型理解，句子该从哪里开始，在什么时候结束，不会在长文本胡言乱语。&lt;/li&gt;&#xA;&lt;li&gt;由于我们在模型的 &lt;code&gt;forward&lt;/code&gt; 里面规定了：训练模式下将 logits 和 labels 进行偏移，所以在 Dataset 里面返回的 x 和 y 就不用额外的进行 shift 了。&lt;/li&gt;&#xA;&lt;li&gt;padding 补充的 token 不参与 loss 计算，所以在 labels 里面把这部分 token 的 id 设为 -100，和 &lt;code&gt;F.cross_entropy(...,ignore_index=-100)&lt;/code&gt; 一致。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;code-block highlight is-closed show-line-numbers  tw-group tw-my-2&#34;&gt;&#xA;  &lt;div class=&#34;&#xA;    &#xA;    tw-flex &#xA;    tw-flex-row&#xA;    tw-flex-1 &#xA;    tw-justify-between &#xA;    tw-w-full tw-bg-bgColor-secondary&#xA;    &#34;&gt;      &#xA;    &lt;button &#xA;      class=&#34;&#xA;        code-block-button&#xA;        tw-mx-2 &#xA;        tw-flex&#xA;        tw-flex-row&#xA;        tw-flex-1&#34;&#xA;      aria-hidden=&#34;true&#34;&gt;&#xA;          &lt;div class=&#34;group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 320 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z&#34;/&gt;&lt;/svg&gt;&lt;/div&gt;&#xA;          &lt;p class=&#34;tw-select-none !tw-my-1&#34;&gt;python&lt;/p&gt;</description>
    </item>
    <item>
      <title>MiniMind 学习指北(二)：Tokenizer</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-tokenizer/</link>
      <pubDate>Fri, 23 Jan 2026 13:47:19 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-tokenizer/</guid>
      <description>&lt;h2 id=&#34;tokenizer&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#tokenizer&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;Tokenizer&lt;/h2&gt;&lt;blockquote&gt;&#xA;  &lt;p&gt;在 CS336 的笔记中我已经完整介绍了一个 Tokenizer 是如何训练并且读取的，详情可见 &lt;a href=&#34;../cs336_assignment1&#34; rel=&#34;&#34;&gt;cs336_assignment1&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&lt;/blockquote&gt;&lt;p&gt;简单来说，训练一个 tokenizer 经过以下步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过正则分词，获得文本中全部 token，将其和 special_tokens 一起记录。&lt;/li&gt;&#xA;&lt;li&gt;不断把文本中出现频率最高的 token_pair 合并得到新 token，然后用新 token 替换文本中原先的 pair。&lt;/li&gt;&#xA;&lt;li&gt;重复上一步直到 vocab 达到指定规模。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;上面的代码我们已经在 CS336 里实现过了，这一次我们通过 Huggingface 的 tokenizers 库直接生成。为了方便阅读，我先从如何得到一个 tokenizer 讲起。&lt;/p&gt;</description>
    </item>
    <item>
      <title>MiniMind 学习指北(一)：Model</title>
      <link>http://xilyfeAAAA.github.io/posts/minimind-model/</link>
      <pubDate>Thu, 22 Jan 2026 13:47:19 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/minimind-model/</guid>
      <description>&lt;p&gt;这是 Minimind 学习指北系列的第一节，整个系列我们大致分为：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;模型实现&lt;/li&gt;&#xA;&lt;li&gt;Tokenizer&lt;/li&gt;&#xA;&lt;li&gt;预训练&lt;/li&gt;&#xA;&lt;li&gt;评估&lt;/li&gt;&#xA;&lt;li&gt;监督微调&lt;/li&gt;&#xA;&lt;li&gt;LoRA&lt;/li&gt;&#xA;&lt;li&gt;强化学习&lt;/li&gt;&#xA;&lt;li&gt;蒸馏&lt;/li&gt;&#xA;&lt;li&gt;推理&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&#xA;&lt;div class=&#34;post-img-view&#34;&gt;&#xA;&lt;a data-fancybox=&#34;gallery&#34; href=&#34;http://img.xilyfe.top/img/20260122135047734.png&#34;&gt;&#xA;&lt;img src=&#34;http://img.xilyfe.top/img/20260122135047734.png&#34; alt=&#34;&#34;  /&gt;&#xA;&lt;/a&gt;&#xA;&lt;/div&gt;&#xA;&lt;/p&gt;&#xA;&lt;h2 id=&#34;rmsnorm&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#rmsnorm&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;RMSNorm&lt;/h2&gt;&lt;h3 id=&#34;前身&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%89%8d%e8%ba%ab&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;前身&lt;/h3&gt;&lt;p&gt;RMSNorm 也是归一化 Normalization 的一种，它的提出是为了解决这样一个问题：当数据分布非常不一致的时候，模型无法很好的学习到数据里面的信息。从数学层面来看，我们举个例子：假设我们有一个函数&#xA;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
