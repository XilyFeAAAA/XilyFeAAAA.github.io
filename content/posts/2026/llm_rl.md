---
title: LLM ä¸­çš„å¼ºåŒ–å­¦ä¹ 
date: 2026-02-19T15:38:09+08:00
featuredImage: http://img.xilyfe.top/img/20260219153922089.png
authors:
  - Xilyfe
series:
  - LLM
tags:
  - å¤§æ¨¡å‹
  - å¼ºåŒ–å­¦ä¹ 
lastmod: 2026-02-25T01:03:40+08:00
---

>åœ¨ CS224N ä¸­å·²ç»å­¦ä¹ äº†ä¸€éƒ¨åˆ†çš„ RLHF ä½†æ˜¯æ„Ÿè§‰éƒ½å¿˜æ‰äº†è€Œä¸”å­¦ä¹ çš„ä¸€çŸ¥åŠè§£ï¼Œè¿™æ¬¡ minimind æ­£å¥½ä¹Ÿéœ€è¦ç”¨åˆ° RL çš„çŸ¥è¯†ï¼Œæ¶‰åŠåˆ° PPOã€DPO å•¥çš„ï¼Œæ‰€ä»¥æ¥å®Œæ•´å­¦ä¹ ä¸€éã€‚è¿™æ¬¡å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯ææ‡‚ LLM ä¸­å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µï¼Œå­¦ä¼šç›®å‰å¸¸ç”¨çš„ PPOã€DPOã€GRPO è¿™å‡ ä¸ªç®—æ³•ï¼Œç„¶åèƒ½è°ƒåº“è¿›è¡Œ RLHFã€‚

åœ¨å­¦ä¹  RLHF ä¹‹å‰å…ˆé—®ä¸€ä¸ªé—®é¢˜ï¼šSFT å’Œ RLHF æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
SFTï¼šç»™å®šäº† prompt å’Œå¯¹åº”çš„ responseï¼Œä¼˜åŒ–LLMç­–ç•¥ï¼Œè®©æ¨¡å‹æ¯ä¸ªtokençš„é¢„æµ‹åˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘çœŸå®çš„äººå·¥ç­”æ¡ˆï¼Œæœ¬è´¨æ˜¯æ¨¡ä»¿å­¦ä¹ ã€‚è€Œ RLHF ç»™å®š prompt å’Œå¯¹åº”å›å¤ä»¥åŠäººç±»åå¥½ï¼Œä¼˜åŒ–LLMç­–ç•¥ï¼Œè®©æ¨¡å‹è¾“å‡ºçš„è¯­å¥ç¬¦åˆäººç±»åå¥½(ç”¨å¥–åŠ±å‡½æ•°é‡åŒ–è¯„ä»·)ã€‚SFTä¹‹åï¼Œæ¨¡å‹å·²ç»ä¼šâ€œæ¨¡ä»¿äººâ€ï¼Œä½†è¿˜ä¸ä¸€å®šâ€œç¬¦åˆäººç±»åå¥½â€ï¼ˆä¾‹å¦‚ç¤¼è²Œæ€§ã€å®‰å…¨æ€§ã€ç®€æ´æ€§ç­‰)ï¼ŒRLHFå°±æ˜¯è¿›ä¸€æ­¥è®©æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½ï¼Œæœ¬è´¨æ˜¯åå¥½å­¦ä¹ ã€‚

ä¹Ÿå°±æ˜¯è¯´ SFT æ˜¯äººç±»å–œæ¬¢æ€ä¹ˆåšä»–å°±æ€ä¹ˆåšï¼ŒRLHF æ˜¯äººç±»åå‘ä»€ä¹ˆä»–å°±æœé‚£ä¸ªæ–¹å‘å­¦ä¹ ã€‚

## å‰ç½®çŸ¥è¯†

### RL in LLM

>å¼ºåŒ–å­¦ä¹ å°±æ˜¯ä¸€ç§æ¨¡å¼ï¼Œå®ƒä»ç¯å¢ƒä¸­è·å–ç»“æœï¼Œç„¶åå¯¹ç»“æœè¿›è¡Œæ‰“åˆ†è·å¾—å¥–åŠ±ï¼Œæœ€åå°†å…¶ä½œä¸ºåé¦ˆä»ä¸­è¿›è¡Œå­¦ä¹ ã€‚å¤§æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ ¸å¿ƒå°±æ˜¯ **å¦‚ä½•æ„é€ ä¸€ç§ loss**ï¼Œæ¥å¯¹æ¨¡å‹è¿›è¡Œæ­£å‘æˆ–è€…åå‘æ¿€åŠ±ã€‚

åœ¨ LLM è®­ç»ƒä¸­ï¼ŒRLHF å’Œ Pretrain æˆ–è€… SFT ä¸åŒã€‚Pretrain å’Œ SFT éƒ½æ˜¯é‡‡ç”¨ Teacher-Forcing çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬éœ€è¦æå‰å‡†å¤‡å¥½é—®é¢˜å’Œç­”æ¡ˆï¼›ä½†æ˜¯ RLHF ä¸­ä¾‹å¦‚ PPO ä¸éœ€è¦å‡†å¤‡è¯­æ–™ï¼Œåªéœ€è¦å‡†å¤‡å¥½é—®é¢˜è®© LLM è¿›è¡Œ next-token çš„é¢„æµ‹ï¼Œé¢„è®­ç»ƒå¥½çš„æ‰“åˆ†æ¨¡å‹ä¼šå¯¹å›ç­”è¿›è¡Œæ‰“åˆ†å¾—åˆ°å¥–åŠ±ã€‚

![image.png](http://img.xilyfe.top/img/20260219160939561.png)


å¤§æ¨¡å‹ç”Ÿæˆåºåˆ—çš„è¿‡ç¨‹å¯ä»¥çœ‹ä½œä¸€ä¸ª **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)**ï¼š
- **Episode (å›åˆ)**ï¼šä»ç»™å‡º Prompt åˆ°ç”Ÿæˆç»“æŸï¼ˆå‡ºç° EOS æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼‰ã€‚
- **Step (æ­¥)**ï¼šç”Ÿæˆæ¯ä¸€ä¸ª Token çš„è¿‡ç¨‹ã€‚
- **Agent (æ™ºèƒ½ä½“)**ï¼šLLM æ¨¡å‹è‡ªèº«ã€‚
- **Environment (ç¯å¢ƒ)**ï¼šå·²ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ã€‚
- **Action ($A_t$)**ï¼šå½“å‰é¢„æµ‹ç”Ÿæˆçš„ Tokenã€‚
- **State ($S_t$)**ï¼šå½“å‰çš„ Prompt + å·²ç”Ÿæˆçš„ Token åºåˆ—ã€‚
- **Reward ($R_t$)**ï¼šç¯å¢ƒï¼ˆæˆ–å¥–åŠ±æ¨¡å‹ï¼‰ç»™å‡ºçš„å³æ—¶åé¦ˆã€‚

ä¾‹å¦‚ï¼Œå›´æ£‹çš„ä¸€å±€ï¼Œè¶…çº§é©¬é‡Œå¥¥æ¸¸æˆä¸­ä»æ¸¸æˆå¼€å§‹åˆ°æ•‘å‡ºå…¬ä¸»çš„è¿‡ç¨‹ï¼Œæˆ–è€…è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸€ä¸ªå¥å­çš„è¿‡ç¨‹ï¼Œè¿™äº›éƒ½æ˜¯ä¸€ä¸ªepisodeã€‚å›´æ£‹ä¸­æŸä½æ£‹æ‰‹çš„ä¸€æ¬¡è½å­ï¼Œè¶…çº§é©¬é‡Œå¥¥æ¸¸æˆä¸­ç©å®¶çš„ä¸€æ¬¡æ“ä½œï¼Œæˆ–è€…è¯­è¨€æ¨¡å‹ç”Ÿæˆå¥å­ä¸­çš„ä¸€ä¸ªtokenï¼Œè¿™äº›éƒ½æ˜¯ä¸€ä¸ªstepã€‚

ç¬¬tä¸ªstepä¸­ï¼Œagentä¸ç¯å¢ƒäº¤äº’åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼ˆå¦‚ä¸Šå›¾ï¼‰ï¼š
1. agentæ”¶åˆ°æ¥è‡ªç¯å¢ƒçš„çŠ¶æ€$S_t$
2. åŸºäºè¯¥çŠ¶æ€Â $S_t$ï¼Œagenté‡‡å–åŠ¨ä½œ$A_t$
3. ç¯å¢ƒè¿›å…¥æ–°çŠ¶æ€ $S_{t+1}$Â 
4. ç¯å¢ƒä¼šç»™agentå¸¦æ¥ä¸€äº›å¥–åŠ± $R_t$

åœ¨ LLM çš„è¯­å¢ƒä¸‹ï¼Œç»™å‡ºä¸€ä¸ª prompt ç”Ÿæˆ response çš„è¿‡ç¨‹å°±æ˜¯ä¸€ä¸ª episodeï¼Œç”Ÿæˆæ¯ä¸€ä¸ª token å°±ç§°ä¸ºä¸€ä¸ª stepã€‚æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ªepisode ä¸­æ‰€æœ‰å¥–åŠ±ä¹‹å’Œèƒ½å¤Ÿè¶Šå¤§è¶Šå¥½ã€‚å› æ­¤ agent çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸€ä¸ª episode ä¸­æ‰€æœ‰å¥–åŠ±ä¹‹å’Œçš„æœŸæœ›ï¼ˆä¹‹æ‰€ä»¥æ˜¯æœŸæœ›è€Œä¸æ˜¯ç²¾ç¡®å€¼ï¼Œæ˜¯å› ä¸ºé‡‡å–åŠ¨ä½œåè¿›å…¥å“ªä¸ªæ–°çŠ¶æ€æ˜¯ç¯å¢ƒè¯´äº†ç®—çš„ï¼Œå…·æœ‰ä¸€å®šçš„éšæœºæ€§ï¼‰ã€‚

### Actor-Critic ç®—æ³•

![image.png](http://img.xilyfe.top/img/20260219165713553.png)

Actor-Critic ç®—æ³•åŒ…å«ä¸¤ä¸ªè§’è‰²ï¼šæ¼”å‘˜ actor å’Œ è¯„åˆ¤å‘˜ criticã€‚åœ¨å¤§æ¨¡å‹çš„è¯­å¢ƒä¸­ï¼Œactor å°±æ˜¯æˆ‘ä»¬ LLMï¼Œå®ƒä¼šå¯¹ prompt é¢„æµ‹ä¸æ–­é¢„æµ‹å‡ºä¸‹ä¸€ä¸ª tokenï¼›critic ä¹Ÿæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒé€šå¸¸éœ€è¦è¾“å…¥ $S_t$ å’Œ $A_t$ ä¸¤ä¸ªå‘é‡ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªæ ‡é‡ä»£è¡¨é¢„æµ‹çš„æ”¶ç›Šã€‚

ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œactor é€šè¿‡æ¸¸æˆæœºçš„ç¯å¢ƒåšå‡ºä¸‹ä¸€ä¸ª actionï¼Œç„¶å critic æ ¹æ® actor çš„åŠ¨ä½œç»™å‡ºè¯„ä»·ï¼Œç„¶å actor æ ¹æ®è¯„ä»·å†è°ƒæ•´åšå‡ºä¸‹ä¸€ä¸ª actionã€‚ä½†æ˜¯è¿™é‡Œçš„ critic æ›´åƒä¸€ä¸ª **é¢„è¨€å®¶**ï¼Œå› ä¸ºCritic çš„æ ¸å¿ƒä½œç”¨æ˜¯ **é¢„æµ‹æœªæ¥çš„é•¿æœŸæœŸæœ›å›æŠ¥**ï¼Œè€Œä¸æ˜¯ä»…ä»…è¯„ä¼°å½“å‰çš„å³æ—¶æ”¶ç›Šã€‚æˆ‘ä»¬ä¹‹å‰è¯´è¿‡ï¼š

>æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ªepisode ä¸­æ‰€æœ‰å¥–åŠ±ä¹‹å’Œèƒ½å¤Ÿè¶Šå¤§è¶Šå¥½

æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯è®©å½“å‰è¿™ä¸€æ­¥å¾—åˆ†æœ€é«˜ï¼Œè€Œæ˜¯è®©æ•´ä¸ª episode çš„ç´¯ç§¯ $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$ æ”¶ç›Šæœ€å¤§åŒ–ã€‚ä½†æ˜¯å› ä¸ºæœªæ¥çš„ $R_{t+2}, R_{t+3}$ ç­‰æ”¶ç›Šåœ¨å½“å‰æ˜¯æœªçŸ¥çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è®© Critical æ¥ä¼°ç®—æœªæ¥çš„æ”¶ç›Šæ€»å’Œã€‚å‡å¦‚å¦‚æœæ²¡æœ‰è¯„è®ºå®¶ï¼Œå°±å¿…é¡»å¾—ç­‰åˆ°episode ç»“æŸæ‰èƒ½çŸ¥é“æ”¶ç›Šæ€»å’Œã€‚

åœ¨ Actor-Critical ä¸­æ¯ä¸ª step ä¼šå‘ç”Ÿä»¥ä¸‹å››ä»¶äº‹ï¼š
1. æ¼”å‘˜æ”¶åˆ°æ¥è‡ªç¯å¢ƒçš„çŠ¶æ€Â $S_t$
2. æ¼”å‘˜ç”ŸæˆåŠ¨ä½œÂ Â ï¼Œç„¶åè¯„è®ºå®¶ä¼°è®¡çŠ¶æ€åŠ¨ä½œä»·å€¼Â $Q(S_t, A_t)$Â ã€‚æ¼”å‘˜ç”¨Â $loss = -\log p(A_t|S_t) Q(S_t, A_t)$Â æ¥æ›´æ–°å‚æ•°
3. ç¯å¢ƒæ”¶åˆ°Â $A_t$Â ä¹‹åç»™å‡ºÂ $S_{t+1}$Â ï¼Œæ›´æ–°å‚æ•°åçš„æ¼”å‘˜ç”¨Â $S_{t+1}$Â ç”ŸæˆÂ $A_{t+1}$
4. ç¯å¢ƒç»™å‡ºÂ $R_t$Â ï¼Œè¯„è®ºå®¶ç”¨Â $loss = [Q(S_{t+1}, A_{t+1}) + R_t - Q(S_t, A_t)]^2$Â æ¥æ›´æ–°å‚æ•°

 å…ˆæ¥ç†è§£ä¸€ä¸‹æ¼”å‘˜æ¨¡å‹çš„ loss å‡½æ•°ã€‚æˆ‘ä»¬å¿½ç•¥ $-log$ï¼ŒæŸå¤±å‡½æ•°é‡Œé¢å«è´Ÿå¯¹æ•°çš„åŸå› æˆ‘ä»¬åœ¨è´Ÿå¯¹æ•°ä¼¼ç„¶å°±å·²ç»äº†è§£äº†ã€‚å½“ Critical é¢„æµ‹ä»·å€¼ $Q(S_t, A_t)>0$ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›è¿™ä¸ª action çš„æ¦‚ç‡åˆ†å¸ƒå°½å¯èƒ½å¤§ï¼ŒActor å°±ä¼šå°½å¯èƒ½æ›´æ–°å‚æ•°æ¥ä½¿ $p(A_t|S_t)$ å˜å¤§ã€‚å¦‚æœé¢„æµ‹ä»·å€¼ $Q(S_t, A_t)<0$ï¼Œé‚£ä¹ˆä¼šå¸Œæœ›å®ƒçš„æ¦‚ç‡åˆ†å¸ƒå°½å¯èƒ½å°ï¼ŒActor æ›´æ–°å‚æ•°ä½¿ $p(A_t|S_t)$ å˜å°ã€‚

é‚£è¯„åˆ¤å‘˜æ¨¡å‹çš„ loss å‡½æ•°å‘¢ï¼Ÿ$Q(S_{t+1}, A_{t+1}) + R_t - Q(S_t, A_t)$ å…¶å®å°±æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·ï¼Œ$Q(S_t, A_t)$ å®é™…ä¸Šç­‰äº $Q(S_{t+1}, A_{t+1}) + \hat{R_t}$ï¼Œä½œå·®å°±èƒ½å¾—åˆ° $R_t - \hat{R_t}$ã€‚æˆ‘ä»¬ä¼šå¸Œæœ›å·®è·çš„ç»å¯¹å€¼å°½å¯èƒ½å°ï¼Œä»¥æ­¤æ¥ä¼˜åŒ–è¯„åˆ¤å‘˜æ¨¡å‹ï¼Œè®©ä»–å°½å¯èƒ½è´´è¿‘çœŸå®çš„å¥–åŠ±ã€‚
### A2C ç®—æ³•

A2C å…¨ç¨‹æ˜¯ Advantage Actor-Criticï¼Œæ˜¯ Actor-Critic ç®—æ³•çš„æ”¹è‰¯ã€‚

å®ƒçš„æ€æƒ³å¾ˆç®€å•ï¼šå‡å¦‚ä½ å’Œä½ çš„æœ‹å‹éƒ½æ˜¯å­¦ç”Ÿï¼Œä½ å¹³æ—¶è€ƒè¯•è€ƒ90åˆ†ï¼Œä»–å¹³æ—¶è€ƒè¯•è€ƒ60åˆ†ã€‚ç»è¿‡ä¸€ä¸ªæœˆçš„æœŸæœ«å¤ä¹ ï¼Œåœ¨æœŸæœ«è€ƒè¯•ä¸­ä½ è€ƒäº†96åˆ†ï¼Œä»–è€ƒäº†95åˆ†ï¼Œä½ è§‰å¾—è°çš„æœŸæœ«å¤ä¹ ç­–ç•¥æ˜¯æˆåŠŸçš„ï¼Ÿæ˜¾ç„¶ä½ æœ‹å‹çš„æœŸæœ«å¤ä¹ ç­–ç•¥æ˜¯æ›´æˆåŠŸçš„ã€‚è™½ç„¶ä½ è€ƒäº†æ›´é«˜çš„åˆ†æ•°ï¼Œä½†è¿™ä¸ªåˆ†æ•°åŸºäºä½ å¹³æ—¶çš„ç§¯ç´¯ï¼Œç›¸å½“äºæ˜¯æ­£å¸¸å‘æŒ¥äº†ã€‚è€Œä½ æœ‹å‹å´æ˜¯è¶…å¸¸å‘æŒ¥ã€‚å› æ­¤å•çœ‹æœŸæœ«ï¼Œä»–çš„å¤ä¹ ç­–ç•¥æ›´å€¼å¾—ä»–å¥½å¥½å¼ºåŒ–ã€‚

åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒA2C å¼•å…¥äº†ä¸€ä¸ª **ä¼˜åŠ¿ adv** çš„æ¦‚å¿µæ¥ä»£æ›¿ä¹‹å‰çš„ $Q$ã€‚å‡è®¾è¯„è®ºå®¶çš„é¢„ä¼°åŠ¨ä½œä»·å€¼ä¸º $Q(S_t,A_t)$ é¢„ä¼°çŠ¶æ€ä»·å€¼ä¸º $V(S_t)$ é‚£ä¹ˆï¼š

$$
\text{Adv} = Q(S_t,A_t) - V(S_t)
$$

- æ¼”å‘˜ $loss=-\log{p(A_t|S_t)Adv(S_t,A_t)}$
- è¯„è®ºå®¶ $loss=Adv^2(S_t,A_t)$ 

è¿™é‡Œçš„ $V(S_t)$ å’Œä¹‹å‰çš„ $Q(S_t,A_t)$ æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿ
é¦–å…ˆ $Q(S_t,A_t)$ æŒ‡çš„æ˜¯ï¼šåœ¨çŠ¶æ€ $S_t$ ä¸‹ï¼Œæ‰§è¡Œç‰¹å®šåŠ¨ä½œ $A_t$ ä¹‹åï¼Œæœªæ¥èƒ½æ‹¿åˆ°çš„æ€»æ”¶ç›Šã€‚æ¯”å¦‚ï¼šå¦‚æœä½ è¿™æ­¥æ£‹èµ°â€˜è·³é©¬â€™ï¼Œä½ æœªæ¥çš„èƒœç‡æ˜¯ 80%ã€‚è€Œ $V(S_t)$ æŒ‡çš„æ˜¯ï¼šåœ¨çŠ¶æ€ $s$ ä¸‹ï¼ŒæŒ‰ç…§å½“å‰çš„ç­–ç•¥ç»§ç»­èµ°ä¸‹å»ï¼Œæ‰§è¡Œä»»ä½• action å¹³å‡èƒ½æ‹¿åˆ°çš„æ€»æ”¶ç›Šã€‚æ¯”å¦‚ï¼šä½ ç°åœ¨çš„ç›˜é¢å¤§ä¼˜ï¼Œå¹³å‡èƒœç‡æ˜¯ 70%ã€‚é‚£å¦‚æœåƒ Actor-Critical æ¨¡å‹ä¸€æ ·é‡‡ç”¨ $Q(S_t,A_t)$ å°±å­˜åœ¨ä¸€ä¸ªé—®é¢˜ã€‚å‡å¦‚ç°åœ¨çš„çŠ¶æ€éå¸¸å¥½ï¼Œä¸ç®¡é€‰å“ªä¸ª action éƒ½èƒ½å¾—åˆ°ä¸€ä¸ªå¾ˆå¥½çš„æ”¶ç›Šï¼Œ$Q(S_t, A_T)$ éƒ½ä¼šå¾ˆå¤§ï¼Œè¿™å°±ä¼šå¯¼è‡´æ¢¯åº¦æ›´æ–°æ–¹å‘ä¸ç¨³å®šã€‚æ‰€ä»¥ A2C é‡‡ç”¨ $Q(S_t,A_t) - V(S_t)$ å°±èƒ½å¾—åˆ° **å½“å‰ç­–ç•¥æ˜¯å¦æ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘**ã€‚

åœ¨ A2C çš„å·¥ç¨‹å®ç°ä¸­é€šå¸¸ä¸å•ç‹¬è®­ç»ƒä¸€ä¸ª $Q$ ç½‘ç»œï¼Œè€Œæ˜¯åˆ©ç”¨ **æ—¶åºå·®åˆ†è¯¯å·® TD Error** æ¥ä»£æ›¿ $Q$ã€‚æˆ‘ä»¬æ ¹æ®è´å°”æ›¼æ–¹ç¨‹ï¼š

$$
Q(S_t, A_t) \approx R_t + \gamma V(S_{t+1})
$$

å°±å¯ä»¥å¾—åˆ°æ–°å…¬å¼ï¼š

$$
\delta_t = R_t + \gamma V(S_{t+1}) - V(S_t)
$$

è¿™æ ·åªéœ€è¦å­¦ä¹ ä¸€ä¸ª $V$ å‡½æ•°ï¼Œå°±èƒ½åŒæ—¶å¾—åˆ°çŠ¶æ€ä»·å€¼ä¼°è®¡å’Œä¼˜åŠ¿ä¼°è®¡ï¼Œæ— éœ€é¢å¤–å­¦ä¹ å¤æ‚çš„ $Q$ å‡½æ•°ã€‚ä½†å¦‚æœåªç”¨ä¸€æ­¥çš„ TD Error ä½œä¸º Advantageï¼Œè™½ç„¶åå·®å°ï¼Œä½†æ³¢åŠ¨å¾ˆå¤§ã€‚ä¸ºäº†å¹³è¡¡å‡†ç¡®åº¦å’Œç¨³å®šæ€§å¼•å…¥äº† GAEã€‚è¿™é‡Œä¸å¯¹å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ç‚¹åšè¿‡å¤šä»‹ç»ï¼ŒGAE ä¸»è¦çš„æ€æƒ³å°±æ˜¯ **ç»¼åˆè€ƒé‡æœªæ¥çš„å˜åŒ–ï¼Œåšä¸€ä¸ªåŠ æƒå¹³å‡**ã€‚

$$
A_t^{GAE} = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \dots + (\gamma\lambda)^{T-t}\delta_T
$$

è¿™é‡Œçš„ $\lambda$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä¸€èˆ¬å– 0.95ï¼š
- å¦‚æœ $\lambda = 0$ï¼ŒGAE å°±é€€åŒ–æˆäº†å•çº¯çš„ TD Errorã€‚
- å¦‚æœ $\lambda = 1$ï¼ŒGAE å°±å˜æˆäº†è’™ç‰¹å¡æ´›é‡‡æ ·ï¼Œç®—å‡ºæ•´æ¡è·¯å¾„çš„æ€»å’Œã€‚

>ä»£ç å®ç°ä¸Šï¼Œç”±äºç®—å‡º t æ—¶åˆ»çš„ $A_t^{GAE}$ éœ€è¦ä¹‹å‰ t+1 æ—¶åˆ»ä¹‹åçš„ $\delta_t$ ï¼Œæ‰€ä»¥éœ€è¦å¯¹ token é€†åºçš„å¤„ç†ã€‚

äºæ˜¯ A2C çš„æ­¥éª¤å³ä¸ºï¼š
1. æ¼”å‘˜æ”¶åˆ°æ¥è‡ªç¯å¢ƒçš„çŠ¶æ€Â $S_t$ï¼Œç”ŸæˆåŠ¨ä½œÂ $A_t$
2. ç¯å¢ƒæ”¶åˆ° $A_t$Â ä¹‹åç»™å‡ºå¥–åŠ±Â $R_t$Â å’Œæ–°çŠ¶æ€Â $S_{t+1}$
3. è¯„è®ºå®¶ä¼°è®¡çŠ¶æ€ä»·å€¼Â $V(S_t)ï¼ŒV(S_{t+1})$ï¼ŒÂ å¹¶è®¡ç®—ä¼˜åŠ¿Â $Adv(S_t, A_t) = A_t^{GAE}$
4. æ¼”å‘˜ç”¨Â $loss = -\log p(A_t|S_t) \text{Adv}(S_t, A_t)$Â æ›´æ–°å‚æ•°
5. è¯„è®ºå®¶ç”¨Â $loss = [\text{Adv}(S_t, A_t)] ^2$Â æ›´æ–°å‚æ•°

### KL æ•£åº¦

åœ¨ RLHF ä¸­å­˜åœ¨ Reward Hacking è¿™ä¸ªæ¦‚å¿µï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å¯èƒ½ä¼šæœç€ Reward å€¾å‘çš„æ–¹å‘èµ°æ·å¾„ã€‚æ¯”å¦‚ prompt æ˜¯ "ä»Šå¤©çš„å¤©æ°”å¦‚ä½•"ï¼Œé‚£ä¹ˆæ¨¡å‹ä¼šç”Ÿæˆå¤©æ°”æƒ…å†µçš„åˆ†æç„¶åå†å‘Šè¯‰ä»Šå¤©çš„å¤©æ°”ï¼Œä¾æ¬¡æ¥è·å–æ›´é«˜çš„ rewardã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨ loss ä¸­å¢åŠ ä¸€é¡¹ï¼Œé¿å…æ¨¡å‹è¿‡åº¦å­¦ä¹ ï¼Œæˆ–è€…è¯´è®©è®­ç»ƒåçš„æ¨¡å‹å’ŒåŸæ¨¡å‹å·®è·å°ä¸€äº›ã€‚RLHF ä¸­å¼•å…¥äº†ä¸€ä¸ª Ref Modelï¼Œè¿™ä¸ªæ¨¡å‹å°±æ˜¯ç»è¿‡ SFT è®­ç»ƒåå†»ç»“çš„æ¨¡å‹ã€‚æˆ‘ä»¬ç”¨å®ƒå’Œæ–°æ¨¡å‹è®¡ç®— KL æ•£åº¦ï¼Œæ¥è¡¡é‡æ¨¡å‹ç›¸è¾ƒäºåŸå…ˆçš„å˜åŒ–ã€‚KL æ•£åº¦çš„è®¡ç®—å…¬å¼ä¸ºï¼š

$$
\text{KL} = \frac{1}{n}\sum_{\text{response}}{log{\frac{p(a|s)}{p_{ref}(a|s)}}}
$$

 é¦–å…ˆï¼Œæˆ‘ä»¬å–‚ä¸€ä¸ª prompt ç»™ Actor æ¨¡å‹ï¼Œè®©å®ƒæ­£å¸¸è¾“å‡ºå¯¹åº”çš„ responseã€‚response ä¸­æ¯ä¸€ä¸ª token éƒ½æœ‰å®ƒå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬æŠŠå®ƒè®°ä¸ºÂ log_probsã€‚æˆ‘ä»¬æŠŠ Actor ç”Ÿæˆçš„"prompt + response" ä»¥ Teacher-Forcing çš„æ–¹å¼å–‚ç»™ Reference æ¨¡å‹ï¼Œé‚£ä¹ˆå®ƒåŒæ ·èƒ½ç»™å‡º response ä¸­æ¯ä¸ª token çš„ log_prob ç»“æœï¼Œæˆ‘ä»¬è®°å…¶ä¸º ref_log_probsã€‚æŠŠè¿™ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä½œå·®ï¼Œç„¶åå†æ±‚å¯¹æ•°ä¹‹å’Œçš„å¹³å‡å€¼ï¼Œå°±æ˜¯ KL æ•£åº¦äº†ã€‚

KL æ•£åº¦çš„æ ‡å‡†å®šä¹‰åº”è¯¥æ˜¯ï¼šå¯¹äºå•ä¸ª token çš„ KL æ•£åº¦æ˜¯è¦å¯¹ vocab ä¸Š **æ¯ä¸€ä¸ª token** çš„æ¦‚ç‡åˆ†å¸ƒä½œå·®ã€‚ä½†æ˜¯åœ¨ RLHF çš„å®é™…å®ç°ä¸­ï¼Œ**KL åªé’ˆå¯¹Â Actor å®é™…ç”Ÿæˆçš„ token è®¡ç®—æ¦‚ç‡å·®**ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—Â `log p_actor(response[t]) âˆ’ log p_ref(response[t])`ã€‚

Hugging Face çš„ trl åº“ä¸­ KL æƒ©ç½šçš„è®¡ç®—é€»è¾‘å¯ä»¥ç®€åŒ–å¦‚ä¸‹ï¼š

```python
# 1. è·å–å½“å‰æ¨¡å‹ç”Ÿæˆ token çš„ log_prob
log_probs = actor_model.get_log_probs(states, actions) 

# 2. è·å–å‚è€ƒæ¨¡å‹ (Ref) ç”ŸæˆåŒæ · token çš„ log_prob
# æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ ref_model è¾“å‡ºæ•´ä¸ª vocab çš„åˆ†å¸ƒï¼Œåªéœ€è¦ gather å¯¹åº” action çš„å€¼
ref_log_probs = ref_model.get_log_probs(states, actions) 

# 3. è®¡ç®— KL ä¼°è®¡
kl = log_probs - ref_log_probs

# 4. åŠ å…¥ Loss
# é€šå¸¸è¿˜ä¼šä¹˜ä¸€ä¸ªç³»æ•° betaï¼Œå¹¶ä¸”æœ‰æ—¶ä¼šå– abs(kl) æˆ–è€…æ ¹æ®æ–¹å‘è°ƒæ•´
loss = ppo_loss + beta * kl 
```

`get_log_probs` å‡½æ•°å†…éƒ¨é€šå¸¸æ˜¯é€šè¿‡ `gather` æ“ä½œï¼Œç›´æ¥ä» logits ä¸­å–å‡ºå¯¹åº” action ç´¢å¼•çš„å€¼ï¼Œè€Œä¸æ˜¯éå†æ•´ä¸ª vocabã€‚

å…¶æ¬¡ï¼Œç›®å‰æœ€æ ‡å‡†çš„åšæ³•æ˜¯å§ KL æ•£åº¦åŠ åœ¨ Reward Function ä¸­ï¼š

$$
r_{\text{final}}(s, a) = r_{\text{original}}(s, a) - \beta \cdot \log \frac{\pi_\theta(a|s)}{\pi_{\text{ref}}(a|s)}
$$

ä¸»è¦åŸå› åœ¨äºï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒKL æ•£åº¦è¢«è§†ä¸ºæ¯ä¸€æ­¥çš„â€œä»£ä»·â€ã€‚å¦‚æœ Value ç½‘ç»œåªå­¦ä¹ åŸå§‹å¥–åŠ±ï¼Œè€Œ Actor å´å—åˆ° KL æƒ©ç½šï¼Œé‚£ä¹ˆ Value ç½‘ç»œä¼°è®¡çš„ $V(s)$ å’Œ Actor å®é™…ç»å†çš„å›æŠ¥å°±ä¸åŒ¹é…ï¼Œå¯¼è‡´ Advantage è®¡ç®—ä¸å‡†ç¡®ï¼ˆBaseline åå·®ï¼‰ã€‚

## PPO

### åŸºç¡€æ¦‚å¿µ

#### è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–

PPO ç®—æ³•å¯ä»¥çœ‹æˆ A2C çš„ä¼˜åŒ–ç‰ˆã€‚A2Cçš„è®­ç»ƒç­–ç•¥æ˜¯ â€œé‡‡æ ·ä¸€æ¬¡ï¼Œæ›´æ–°ä¸€æ¬¡ï¼Œç„¶åæ‰”æ‰æ•°æ®â€ï¼Œè¿™å°±å¯¼è‡´æ•ˆç‡å¾ˆä½ï¼Œæ¯æ‰¹æ•°æ®åªèƒ½ç”¨ä¸€æ¬¡ã€‚PPO é‡‡ç”¨ **è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**ï¼Œå®ƒä¼šå¯¹ä¸€æ‰¹æ•°æ®å…ˆè¿›è¡Œé‡‡æ ·å¾—åˆ° `logprob`ï¼Œ`ref_logprob`ï¼Œ`rewards`ï¼Œ`advantages` ç­‰æ•°æ®ï¼Œç„¶åè¿›è¡Œ `ppo_epochs` æ¬¡å¾ªç¯ã€‚æ¯æ¬¡å¾ªç¯å†…ï¼Œå˜åŒ–çš„åªæœ‰æ¦‚ç‡åˆ†å¸ƒ `logprob` å’Œ `ref_logprob` å’Œ `values`ï¼Œä¼˜åŠ¿å¥–åŠ±è¿™äº›éƒ½å›ºå®šé‡‡ç”¨ç¬¬ä¸€æ¬¡å¾—åˆ°çš„æ•°æ®ã€‚

>ä¸¾ä¸ªä¾‹å­ï¼šA2C å°±åƒåœ¨è¡¨æ¼”ç°åœºï¼Œä½ ä¸€è¾¹æ¼”ï¼Œå¯¼æ¼”ä¸€è¾¹å–Šâ€œå¥½â€æˆ–â€œåâ€ï¼Œç„¶åä½ å¾—åˆ°åé¦ˆå°±ä¿®æ”¹ã€‚æ”¹å®Œä¹‹åï¼Œåˆšæ‰æ¼”çš„é‚£æ®µæˆå°±æ²¡ç”¨äº†ï¼Œä½ å¿…é¡»é‡æ–°æ¼”ä¸€æ®µï¼Œå¯¼æ¼”æ‰èƒ½ç»™æ–°åé¦ˆã€‚è€Œ PPO æ›´åƒ **å¤ç›˜å½•åƒ**ï¼Œä½ å…ˆæ¼”ä¸€æ®µæˆå½•ä¸‹æ¥ï¼Œæ¥ä¸‹æ¥çš„ 4 ä¸ª Epoch ä½ ååœ¨ç›‘è§†å™¨å‰ï¼Œå¯¹ç€è¿™æ®µå½•åƒåå¤ç¢ç£¨ã€‚ç¬¬ä¸€éæ ¹æ®åé¦ˆæ”¹ä¸€ç‚¹ï¼Œç¬¬äºŒéåœ¨ç¬¬ä¸€éæ”¹åŠ¨çš„åŸºç¡€ä¸Šï¼Œå†å¯¹ç€å½•åƒå¾®è°ƒã€‚

ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨åŒä¸€æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ¢¯åº¦ä¸‹é™ï¼Œè¦ä¿æŒ `advantage` å’Œ `rewards` ä¸å˜ï¼Ÿæˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œåˆ†æã€‚
é¦–å…ˆæˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­å¸®æˆ‘ä»¬æŠ½è±¡çš„ç†è§£ä¸€ä¸‹ã€‚å‡å¦‚ä½ æ˜¯ Kobeï¼Œç›´å‡æœºå æ¯å‰è¿˜åœ¨æ¹–äººæ‰“çƒå½•ä¸‹äº†ä¸€ç›˜æ¯”èµ›ã€‚å¦‚ä»Šåœ¨å¤©ä¸Šå¤ç›˜è¿™åœºçƒèµ›ï¼Œå³ä¾¿ä½ ç°åœ¨çƒæŠ€è¿›æ­¥äº†ï¼Œå›çœ‹å½“å¹´çš„å½•åƒï¼Œæ¯ä¸€ä¸ªè¿›çƒåœ¨å½“æ—¶çš„ä»·å€¼ï¼ˆAdvantageï¼‰æ˜¯å®¢è§‚äº‹å®ï¼Œä¸éšä½ ç°åœ¨çš„æ°´å¹³å˜åŒ–ã€‚å…¶æ¬¡ï¼Œå¦‚æœæˆ‘ä»¬ä¸å›ºå®š advantage å’Œ rewardsï¼Œç”¨æ–°çš„ç­–ç•¥æ¥æ›´æ–° advantage å’Œ rewardsï¼Œå¯èƒ½é€ æˆç­–ç•¥åç¦»å¤ªè¿œï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆæˆ–ä¸ç¨³å®šã€‚å¹¶ä¸”æˆ‘ä»¬è®¡ç®—çš„ adv å’Œ reward éƒ½æ˜¯åŸºäº old actor model å¾—åˆ°çš„ï¼Œå‡å¦‚æ¯ä¸ª epoch éƒ½é‡æ–°è®¡ç®—æ–°çš„ adv å’Œ rewardï¼Œç”±äº action æ˜¯åœ¨æ—§çš„ model ä¸Šå¾—åˆ°äº†å°±ä¼šä¸åŒ¹é…ï¼Œå˜æˆ off-policyã€‚è€Œ ppo æ˜¯ on-policyï¼Œclip æœºåˆ¶ï¼ˆåæ–‡ä¼šæåˆ°ï¼‰å°±ä¼šå´©æºƒäº†ã€‚

#### æ›´æ–°çº¦æŸ

ä»–ä¸»è¦æ˜¯è§£å†³äº† A2C è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚åœ¨ A2C ä¸­ï¼Œå¦‚æœå­¦ä¹ ç‡è®¾å¾—ç¨å¾®å¤§ä¸€ç‚¹ï¼Œä¸€æ¬¡æ›´æ–°å¯èƒ½è®©ç­–ç•¥å‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ï¼Œå¦‚æœ Actor çªç„¶å­¦åˆ°äº†ä¸€ä¸ªæå…¶ç³Ÿç³•çš„åŠ¨ä½œï¼Œæ•´ä¸ªç­–ç•¥å¯èƒ½ç¬é—´å´©å¡Œã€‚

é‚£æˆ‘ä»¬å¦‚ä½•è¡¡é‡ç­–ç•¥çš„å˜åŒ–å¹…åº¦å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥çœ‹ä¸¤ä¸ªç­–ç•¥æ‰§è¡Œç›¸åŒåŠ¨ä½œå¾—åˆ°ç»“æœçš„å·®åˆ«ã€‚åœ¨ LLM ä¸­å°±æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ›´æ–°å‚æ•°å‰åçš„æ¨¡å‹ p å’Œ p' éƒ½è¾“å…¥ tokenï¼Œå°±èƒ½å¾—åˆ°çš„ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒ $p(A_t|S_t)$ å’Œ $p'(A_t|S_t)$ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ¯”ç‡ $r_t(\theta)$ï¼Œè¡¨ç¤º**æ–°ç­–ç•¥**å’Œ**æ—§ç­–ç•¥**äº§ç”ŸæŸä¸ªåŠ¨ä½œçš„æ¦‚ç‡æ¯”ï¼š

$$
r_t(\theta) = \frac{p(a_t|s_t)}{p'(a_t|s_t)}
$$

- å¦‚æœ $r_t > 1$ï¼šè¯´æ˜è¿™ä¸ªåŠ¨ä½œåœ¨æ–°ç­–ç•¥ä¸­å‡ºç°çš„æ¦‚ç‡å˜å¤§äº†ã€‚
- å¦‚æœ $r_t = 1$ï¼šè¯´æ˜æ–°æ—§ç­–ç•¥å®Œå…¨ä¸€æ ·ã€‚

è¿›è€Œæœ‰äº†æ¼”å‘˜çš„ loss å‡½æ•°ï¼š

$$
loss = -\frac{p(A_t|S_t)}{p'(A_t|S_t)} \text{Adv}(S_t, A_t)
$$

æˆ‘ä»¬ä»æ¢¯åº¦æ›´æ–°çš„è§’åº¦æ€è€ƒä¸€ä¸‹è¿™ä¸ªå…¬å¼çš„æ„ä¹‰ï¼Œå…ˆæŠŠå…¬å¼æ”¹ä¸€ä¸‹ $loss = -p(A_t|S_t) \times \frac{\text{Adv}(S_t, A_t)}{p'(A_t|S_t)}$ï¼Œç”±äº adv å’Œ pâ€˜ éƒ½ä¸åœ¨è®¡ç®—å›¾é‡Œé¢ä¸å›ä¼ æ¢¯åº¦ï¼Œæ‰€ä»¥æ¢¯åº¦è¡¨è¾¾å¼ä¸º $\frac{\partial{loss}}{\partial{p}}=- \frac{\text{Adv}(S_t, A_t)}{p'(A_t|S_t)}$ã€‚å‡è®¾æŸæ¬¡ action çš„ $Adv>0$ï¼Œä¹Ÿå°±æ˜¯å†³ç­–ä¼˜äºå¹³å‡æ°´å¹³ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›æé«˜è¿™ä¸ª action çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯å¢åŠ  $p(A_t|S_t)$ã€‚å‡å¦‚åŸå…ˆ $p'(A_t|S_t)$ ä¹Ÿå¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯åŸå…ˆæ¨¡å‹ä¹Ÿè®¤ä¸ºåº”è¯¥æ‰§è¡Œè¿™ä¸ª actionï¼Œé‚£ä¹ˆæ¢¯åº¦çš„ç»å¯¹å€¼å°±ä¼šå˜å°ï¼Œä¸ä¼šåšå‡ºéå¸¸å¤§æ”¹å˜ã€‚å‡å¦‚åŸå…ˆ $p'(A_t|S_t)$ å¾ˆå°ï¼Œæ—§ç­–ç•¥è§‰å¾—è¿™ä¸ªåŠ¨ä½œå‡ ä¹ä¸å¯èƒ½å‘ç”Ÿï¼Œä½†å®é™…é‡‡æ ·å‡ºæ¥å‘ç°è¿™ä¸ªåŠ¨ä½œæ•ˆæœå‡ºå¥‡çš„å¥½ï¼ˆä¹Ÿå°±æ˜¯Adv å¾ˆå¤§ï¼‰ã€‚é‡è¦æ€§é‡‡æ ·å…¬å¼ $\frac{p}{p'}$ ä¼šè®¤ä¸ºï¼šè¿™æ˜¯ä¸€ä¸ªè¢«æ—§ç­–ç•¥ä¸¥é‡ä½ä¼°çš„å¥½ actionï¼Œäºæ˜¯ç®—æ³•ä¼šè¯•å›¾å‰§çƒˆåœ°æé«˜ $p(A_t|S_t)$ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»é™åˆ¶äº†ç­–ç•¥Â $p(A_t|S_t)$Â çš„æ›´æ–°å¹…åº¦ï¼Œä½†è¿˜ç¼ºå°‘ä¸€ä¸ªâ€œç†”æ–­æœºåˆ¶â€ã€‚ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå°±æ˜¯ä¸‡ä¸€ç­–ç•¥çš„æ›´æ–°å¹…åº¦è¿˜æ˜¯å¤ªå¤§äº†ï¼Œæˆ‘ä»¬è¦åœæ­¢ç­–ç•¥çš„å‚æ•°æ›´æ–°ã€‚PPOçš„åšæ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºÂ $\frac{p(A_t|S_t)}{p'(A_t|S_t)}$Â è¡¡é‡äº†æ—§ç­–ç•¥å’Œç°è¡Œç­–ç•¥ä¹‹é—´å·®å¼‚ï¼Œæ‰€ä»¥å¯ä»¥ä¸ºå®ƒè®¾ç½®ä¸¤ä¸ªé˜ˆå€¼ã€‚ä¸ºäº†æ–¹ä¾¿æè¿°ï¼Œæˆ‘ä»¬ä»¤Â $r(A_t, S_t) = \frac{p(A_t|S_t)}{p'(A_t|S_t)}$ï¼š

- å½“ Adv å¤§äº 0 æ—¶ï¼Œè‹¥ r å¤§äº 1.2ï¼Œåˆ™åœæ­¢å‚æ•°æ›´æ–°
- å½“ Adv å°äº 0 æ—¶ï¼Œè‹¥ r å°äº 0.8ï¼Œåˆ™åœæ­¢å‚æ•°æ›´æ–°

è¯¶ï¼Œé‚£ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”¨ç®¡ Adv å¤§äº 0 å’Œ r å°äº 0.8 çš„æƒ…å†µï¼Ÿæˆ–è€… Adv å°äº 0 æˆ–è€… r å¤§äº 1.2 çš„æƒ…å†µï¼ŸAdv å¤§äº 0 çš„æƒ…å†µè¯´æ˜å½“å‰ç­–ç•¥æ˜¯å¥½çš„ï¼Œå¦‚æœ r å°äº 0.8 è¯´æ˜ï¼šè¿™ä¸ªç­–ç•¥æ˜¯å¥½çš„ï¼Œæ—§æ¨¡å‹åå‘è¿™ä¸ªç­–ç•¥ï¼Œä½†æ˜¯æ–°æ¨¡å‹ä¸æ€ä¹ˆåå‘è¿™ä¸ªç­–ç•¥äº†ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›èƒ½å°½å¯èƒ½æœç°åœ¨è¿™ä¸ªæ–¹å‘æ¥æ›´æ–°å‚æ•°ï¼Œæ‰€ä»¥ä¸ä¼šè¿›è¡Œ $max(r, 0.8)$ã€‚åŒæ · Adv å°äº 0 çš„æƒ…å†µè¯´æ˜å½“å‰ç­–ç•¥ä¸æ€ä¹ˆè¡Œï¼Œå¦‚æœ r å¤§äº 1.2 åˆ™è¯´æ˜è¿™ä¸ªä¸å¥½çš„ç­–ç•¥ç°åœ¨å¾ˆçœ‹å¥½ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›åŠ å¤§åŠ›åº¦æ›´æ–°å‚æ•°æ¥é¿å…è¿™ä¸ª actionï¼Œäºæ˜¯ä¸åº”è¯¥é™åˆ¶æ›´æ–°çš„å¹…åº¦ã€‚

è¿™ç§ç†”æ–­æœºåˆ¶å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
loss = -\min(r(A_t, S_t) \text{Adv}(S_t, A_t),\ \text{clip}(r(A_t, S_t) , 0.8, 1.2) \text{Adv}(S_t, A_t))
$$

- Adv å¤§äº 0ï¼Œr å¤§äº 1.2ï¼šmin æ“ä½œå°±ä¼šå–å³è¾¹çš„å€¼ï¼Œæ­¤æ—¶ loss ä¸­å°±åªå‰©å¸¸é‡äº†ï¼Œä¸äº§ç”Ÿä»»ä½•æ¢¯åº¦ï¼›è€Œ r æ— è®ºå¤šå°éƒ½è¿˜æ˜¯ä¼šäº§ç”Ÿæ¢¯åº¦ã€‚
- Adv å°äº 0ï¼Œr å°äº 0.8ï¼šmin æ“ä½œå°±ä¼šå–å³è¾¹çš„å€¼ï¼Œæ­¤æ—¶ loss ä¸­å°±åªå‰©å¸¸é‡äº†ï¼Œä¸äº§ç”Ÿä»»ä½•æ¢¯åº¦ï¼›è€Œ r æ— è®ºå¤šå¤§éƒ½è¿˜æ˜¯ä¼šäº§ç”Ÿæ¢¯åº¦

ç†”æ–­æœºåˆ¶å°±æˆåŠŸäº†ã€‚

#### è¯„è®ºå®¶ loss

åœ¨ä¹‹å‰æˆ‘ä»¬æè¿‡ï¼ŒA2C çš„æŸå¤±å‡½æ•°æ˜¯ $loss = [\text{Adv}(S_t, A_t)] ^2$ã€‚ä½†æ˜¯åœ¨ PPO ä¸­ï¼Œadvantages åœ¨ optimization é˜¶æ®µå°±åº”è¯¥ç”Ÿæˆäº†ï¼Œå¹¶ä¸”åœ¨åé¢å¤šè½®è®­ç»ƒä¸­ä¸å˜æ˜¯ä¸€ä¸ªå®šå€¼ï¼Œé‚£ä¹ˆè¿™ä¸ª loss å®Œå…¨ä¸ä¾èµ–ä¸æ–°çš„ critic æ¨¡å‹çš„å‚æ•°ï¼Œæ— æ³•æ›´æ–° value modelã€‚é‚£æˆ‘ä»¬åº”è¯¥å¦‚ä½•ä¿®æ”¹ critic loss å‘¢ï¼Ÿ

æˆ‘ä»¬æ¥è·Ÿç€æœ€è‡ªç„¶çš„æ€è€ƒé¡ºåºï¼Œé¦–å…ˆæˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯è®© critic çš„è¾“å‡º $V(s)$ å¿…é¡»å°½å¯èƒ½æ¥è¿‘çœŸå®çš„çŠ¶æ€ä»·å€¼ï¼š

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s),\ \tau \sim P} \Big[ \sum_{k=0}^\infty \gamma^k r_{t+k} \Big]
$$

å¦‚æœ value ä¼°å¾—å‡†ï¼Œadvantage = q - v å°±ä¼šä½æ–¹å·®ï¼Œactor å°±èƒ½ç¨³å®šåœ°çŸ¥é“â€œå“ªä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘â€ã€‚ æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è®© $V(S_t)$ ä¸æ–­é€¼è¿‘è¿™ä¸ªâ€œçœŸå®å¹³å‡å›æŠ¥â€ã€‚ä½†æ˜¯çœŸå® $V(S_t)$ æ ¹æœ¬æ‹¿ä¸åˆ°ï¼Œäºæ˜¯æˆ‘ä»¬æƒ³åˆ°çŠ¶æ€ä»·å€¼ $V(S_t)$ æ­£å¥½ç­‰äºæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„åŠ¨ä½œä»·å€¼ $Q(S_t,A_t)$ åœ¨å½“å‰ç­–ç•¥ä¸‹çš„æœŸæœ›ï¼š

$$
V^\pi(s) \equiv \mathbb{E}_{a \sim \pi(\cdot|s)} \big[ Q^\pi(s,a) \big]
$$

ç”±äºæˆ‘ä»¬çš„ advantage æœ¬èº«å®šä¹‰å°±æ˜¯ $Q(S_t,A_t) - V(S_t)$ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§»é¡¹å¯ä»¥å¾—åˆ° $Q(s,a) = V(s) + \text{Adv}(s,a)$ã€‚åœ¨ PPO ä¸­ï¼Œæˆ‘ä»¬è™½ç„¶æ²¡æœ‰è®­ç»ƒå•ç‹¬çš„ Q ç½‘ç»œï¼Œä½†æˆ‘ä»¬ç”¨ GAE ç®—å‡ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ Advantage ä¼°è®¡ï¼š

$$
A_t^{\text{GAE}} \approx Q(s_t, a_t) - V_{\text{old}}(s_t)
$$

å› æ­¤ï¼š

$$
Q(s_t, a_t) \approx V_{\text{old}}(s_t) + A_t^{\text{GAE}}
$$

æˆ‘ä»¬æŠŠè¿™ä¸ªè¿‘ä¼¼å€¼èµ·ä¸ªåå­—å« returnsï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬ç›®å‰èƒ½å¾—åˆ°çš„æœ€å¥½çš„ $Q(S_t,A_t)$ é‡‡æ ·ä¼°è®¡ã€‚å› æ­¤å°±æœ‰äº† critic lossï¼š

$$
\text{loss}_{\text{critic}} = \left( V_{\text{new}}(s) - \text{returns} \right)^2
$$

### trl åº“æºç åˆ†æ

`trl.experiment.ppo.PPOTrainer.train()` æ–¹æ³•å†…éƒ¨ä¾æ¬¡è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼š
1. rollout é˜¶æ®µï¼šå°†æ•°æ®é›†çš„ prompt ä¼ ç»™ actor é‡‡æ · responseï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† prompt+response çš„é—®ç­”å¯¹ã€‚
2. evaluation é˜¶æ®µï¼šç”¨ reward æ¨¡å‹ç»™è¿™ä¸ªé—®ç­”å¯¹æ‰“åˆ†æ•° `scores`ï¼Œæ³¨æ„ **è¿™ä¸ªåˆ†æ•°æ˜¯åºåˆ—çº§çš„è€Œä¸æ˜¯ token çº§çš„**ã€‚
3. optimization é˜¶æ®µï¼šæŠŠ prompt+response ç”¨ Teacher-Forcing çš„æ–¹å¼é€å…¥ refã€actor å’Œ critic æ¨¡å‹å¾—åˆ° response ä¸­æ¯ä¸ª token çš„æ¦‚ç‡ `ref_logprob` å’Œ `old_logprob`ï¼Œä»¥åŠé€ token çš„é¢„æœŸæ”¶ç›Š `old_values`ã€‚æ ¹æ®ä¹‹å‰è®¡ç®—å‡ºçš„æ•´ä¸ªåºåˆ—çš„ rewardï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºæ¯ä¸ª token å¯¹åº”çš„ rewardï¼Œè¿™æ · advantage ä¹Ÿå°±è®¡ç®—å‡ºæ¥äº†ã€‚
4. é‡å¤ ppo_epochs ä¸ªé˜¶æ®µï¼Œä¸æ–­æŠŠ prompt+response ç”¨ Teacher-Forcing çš„æ–¹å¼ä¼ å…¥ actor å¾—åˆ°æ¯ä¸ª token **æ–°çš„æ¦‚ç‡åˆ†å¸ƒ**ï¼ŒæŠŠ response ä¼ å…¥ critic å¾—åˆ° valuesã€‚ç„¶ååˆ©ç”¨ä¹‹å‰ optimization é˜¶æ®µå¾—åˆ°çš„ reward å’Œ advantages æ¥è®¡ç®— actor å’Œ critic çš„ lossï¼Œæ›´æ–°è¿™ä¸¤ä¸ªæ¨¡å‹ã€‚

æˆ‘å€Ÿç”¨çŸ¥ä¹çš„å‡ å¼ å›¾ç‰‡æ¥å›¾è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼š

![image.png](http://img.xilyfe.top/img/20260224122122139.png)


å‰é¢æˆ‘ä»¬æåˆ°ï¼Œevalution é˜¶æ®µè®¡ç®—çš„ reward scores æ˜¯åºåˆ—çº§çš„ï¼Œä½†æ˜¯ PPO åœ¨æ¯ä¸ª stepï¼ˆå¯¹åº”ç”Ÿæˆåºåˆ—ä¸­çš„æ¯ä¸ªtokenï¼‰éƒ½éœ€è¦è®¡ç®— advantage æ¥æ›´æ–° actor modelï¼Œè¿™æ ·ä¸æ˜¯çŸ›ç›¾äº†å—ï¼Ÿ
å®é™…ä¸Š reward æ¨¡å‹åœ¨è®¡ç®—åºåˆ—çº§ reward çš„æ—¶å€™æ²¡æœ‰åŠ å…¥ kl æ•£åº¦ï¼Œè¿™æ—¶å€™è®¡ç®—å¾—åˆ°åˆ†æ•°æˆ‘ä»¬å«åš `scores`ã€‚åœ¨æ¯ä¸€ä¸ª stepï¼Œæˆ‘ä»¬é€šè¿‡ `scores`ï¼Œ`ref_logprob`ï¼Œ`old_logprob` è®¡ç®—å¾—åˆ°è¿™ä¸ª token çš„ rewardï¼Œ$reward = scores - \beta*kl(old\_logprob, ref\_logprob)$ï¼Œæœ€åç”¨è¿™ä¸ª token å¯¹åº”çš„ value å’Œ reward è®¡ç®— advantageã€‚

#### rollout

```python
batch["response"] = []
query_batch = batch["input_ids"]
for query in  query_batch:
	gen_len = output_length_sample()
	generation_kwargs["max_new_tokens"] = gen_len
	resp = ppo_trainer.generate(query, **generation_kwargs)
	batch["response"].append(resp.squeeze()[-gen_len:])
```

`output_length_sample()` ä½œç”¨æ˜¯ **ä¸ºæ¯ä¸ªç”Ÿæˆè¯·æ±‚åŠ¨æ€é‡‡æ ·ä¸€ä¸ªç”Ÿæˆé•¿åº¦**ï¼Œè¿™æ ·å…·æœ‰éšæœºæ€§æˆ–å¯æ§åˆ†å¸ƒï¼Œä¸æ˜¯å›ºå®šæ­»çš„é•¿åº¦ã€‚

#### evaluation

```python
texts = [q + r for q, r in zip(batch["query"], batch["response"])]
reward_out = reward_model(texts)
scores = [torch.tensor(output[1]["score"]) for output in reward_out]
```

#### optimization

```python
old_logprobs, _, values, masks = self.batched_forward_pass(actor_model, queries, responses)
ref_logprobs, *_ = self.batched_forward_pass(ref_model, queries, responses)
```

ç”±äºæ˜¯ batch è®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦è®°å½•ä¸‹ padding ä½ç½®æ–¹ä¾¿åé¢è¿›è¡Œé®ç›–ã€‚

```python
rewards, non_score_rewards = [], []
for score, old_logprob, ref_logprob in zip(scores, old_logprobs, ref_logprobs):
	kl = old_logprob - ref_logprob
	
	non_score_reward = -self.kl_ctl * kl
	non_score_rewards.append(non_score_reward)
	
	reward = non_score_reward.clone()
	last_non_masked_index = mask.nonzero()[-1]
	reward[last_non_masked_index] += score
	rewards.append(reward)
```

å‰é¢æåˆ°è¿‡ï¼ŒAdvantage é‡‡ç”¨äº† GAE æ‰€ä»¥éœ€è¦é€†åºä»åå¾€å‰è®¡ç®—ï¼š

```python
advantanges = []
for t in reversed(range(gen_len)):
	value_t1 = values[:, t+1] if t < gen_len - 1 else 0.0
	delta = rewards[:, t] + self.gamma * value_t1 - values[:, t]
	adv_t = delta + self.gamma * self.lam * adv_t
	advantages.append(adv_t)

advtanges = torch.stack(advantages[::-1])
tgt_return = advantages + values
```

è¿›è¡Œ `ppo_epochs` è½®è®­ç»ƒï¼Œæ¯è½®è®­ç»ƒ minibatch æ¡æ•°æ®ï¼š

```python
for epoch in range(ppo_epochs):
	for batch in minibatch:
		logprobs, logits, values, _ = self.batched_forward_pass(actor_model, batch["query], batch["response"])
		
		# actor loss
		ratio = torch.exp(logprobs - old_logprobs)
		pg_losses = -advantages * ratio
		pg_losses_2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0)
		loss = torch.max(pg_losses, pg_losses_2).mean()
		
		# critic loss
		value_pred_clipped = old_values + torch.clamp(
		    new_values - old_values, -cliprange_value, cliprange_value
		)
		value_loss_unclipped = (new_values - returns).pow(2)
		value_loss_clipped   = (value_pred_clipped - returns).pow(2)
		value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()
```

### ä»£ç å®æˆ˜

æˆ‘è¿™æ¬¡é€‰æ‹©ç›´æ¥å¤ç° bilibili ä¸€ä¸ª up ä¸»çš„ ppo é¡¹ç›® [owenliang/hf-ppo](https://github.com/owenliang/hf-ppo/blob/main/README.md) -  è®©å¤§æ¨¡å‹å­¦ä¼šè¯´è„è¯ã€‚ç”±äºé‡‡ç”¨çš„æ˜¯ Qwen çš„åŸºæ¨¡ä¸å¤ªå¯èƒ½è¾“å‡ºè„è¯ï¼Œç›´æ¥åœ¨ base æ¨¡å‹ä¸Šè¿›è¡Œ ppo å¾ˆéš¾è®­ç»ƒèµ·æ¥ï¼Œæ‰€ä»¥æˆ‘å…ˆç”¨æ•°æ®é›†å¯¹ base æ¨¡å‹è¿›è¡Œ sftï¼Œç„¶ååœ¨ sft çš„åŸºç¡€ä¸Šè¿›è¡Œ ppoï¼Œè¿™æ ·å°±èƒ½å®Œæˆæ•´ä¸ªæµç¨‹ã€‚

è¿™æ¬¡æ•´ä½“çš„è®¡åˆ’å°±æ˜¯å…ˆå¯¹ Qwen çš„åŸºæ¨¡è¿›è¡Œ sftï¼Œç„¶ååœ¨è¿™ä¸ªåŸºç¡€ä¸Šè®­ç»ƒå‡º reward æ¨¡å‹ã€‚ç”¨ sft æ¨¡å‹å½“ policy å’Œ ref_policyï¼Œç”¨ base æ¨¡å‹å½“ valueï¼Œä»¥æ­¤è¿›è¡Œ ppoã€‚

#### SFT

```python
import datetime
import datasets
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTConfig, SFTTrainer

SEED = 14424
SYSTEM_PROMPT = ""

model_name = "Qwen/Qwen3-0.6B"
model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)
model_dir = snapshot_download(model_name, cache_dir="./checkpoint/base")

model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="cuda",
    dtype=model_dtype
)
tokenizer = AutoTokenizer.from_pretrained(model_dir)


def pre_process(example: dict) -> dict:
    return {
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["chosen"]},
        ]
    }


dataset_dir = "./dataset/btfChinese_DPO.jsonl"
pre_dataset = datasets.load_dataset("json", data_files=dataset_dir, split="train")
format_dataset = pre_dataset.map(pre_process, remove_columns=pre_dataset.column_names).train_test_split(test_size=0.2, seed=SEED)

sft_config = SFTConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/sft",
    logging_dir=f"./tensorboard/sft/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    fp16=(model_dtype == torch.float16),
    bf16=(model_dtype == torch.bfloat16),
    num_train_epochs=2,
    save_strategy="no",
    eval_steps=100,
    logging_steps=1,
    max_length=500,
    packing=False
)
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    processing_class=tokenizer
)

trainer.train()
trainer.save_model(sft_config.output_dir)
```

sft çš„ä»£ç åº”è¯¥å¾ˆç†Ÿæ‚‰äº†ï¼Œå”¯ä¸€å¯ä»¥æä¸€æçš„å°±æ˜¯ SFTTrainer é‡Œé¢çš„ `processing_class` å‚æ•°ã€‚ è¿™ä¸ªå‚æ•°æ˜¯æ–°ç‰ˆ huggingface åº“åŠ å…¥ç»™å¤šæ¨¡æ€llmçš„ã€‚å¦‚æœæ˜¯ NLP ä»»åŠ¡ï¼Œé‚£ä¹ˆä¼ å…¥çš„å°±æ˜¯ tokenizerï¼›å¦‚æœæ˜¯å¤šæ¨¡æ€ï¼Œé‚£ä¹ˆä¼ å…¥çš„æ˜¯ Processor å¯¹è±¡ï¼Œé‡Œé¢åŒ…æ‹¬tokenizerï¼ŒImageProcessor ç­‰ç­‰ã€‚

#### Reward

```python
import datetime

import datasets
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from trl import RewardConfig, RewardTrainer

SEED = 14424
SYSTEM_PROMPT = ""

model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)
model_dir = "./checkpoint/sft"

model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=1)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

def pre_process(example: dict) -> dict:
    return {
        "chosen": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["chosen"]},
        ],
        "rejected": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["rejected"]},
        ]
    }

dataset_dir = "./dataset/btfChinese_DPO.jsonl"
pre_dataset = datasets.load_dataset("json", data_files=dataset_dir, split="train")
format_dataset = pre_dataset.map(pre_process, remove_columns=pre_dataset.column_names).train_test_split(test_size=0.2, seed=SEED)


rm_config = RewardConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/reward",
    logging_dir=f"./tensorboard/reward/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    fp16=(model_dtype == torch.float16),
    bf16=(model_dtype == torch.bfloat16),
    num_train_epochs=1,
    save_strategy="no",
    logging_steps=1,
    max_length=512
)

trainer = RewardTrainer(
    model=model,
    args=rm_config,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    processing_class=tokenizer
)

trainer.train()
trainer.save_model(rm_config.output_dir)
```

è®­ç»ƒ Reward æ¨¡å‹éœ€è¦å¯¹æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œå¤„ç†ã€‚é¦–å…ˆ Reward æ¨¡å‹æˆ‘ä»¬è¦ç”¨ `AutoModelForSequenceClassification` è¿›è¡ŒåŠ è½½ï¼Œè¿™ä¸ªç±»ä¼šå†»ç»“ä¼ å…¥çš„åŸºæ¨¡ï¼Œç„¶åå»æ‰æ¨¡å‹çš„ lm_head åŠ å…¥ä¸€ä¸ª linear å±‚ï¼ŒæŠŠ `hidden_size` æ˜ å°„åˆ°æˆ‘ä»¬è®¾ç½®çš„ `num_labels=1`ï¼Œæœ€ç»ˆå°±èƒ½å¾—åˆ°ä¸€ä¸ª reward åˆ†æ•°äº†ã€‚ç„¶åæ•°æ®é›†éœ€è¦å¤„ç†å¾—åˆ°ä¸€ä¸ªæ­£åä¾‹ï¼Œä¹Ÿå°±æ˜¯å­—å…¸é‡Œé¢éœ€è¦åŒ…å« chosen å’Œ rejectedã€‚

#### PPO

```python
from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer
from modelscope.hub.snapshot_download import snapshot_download
from trl.experimental.ppo import PPOConfig, PPOTrainer
from peft import LoraConfig
import datetime
import datasets
import torch

SEED = 14424
SYSTEM_PROMPT = ""

model_name = "Qwen/Qwen3-0.6B"
model_dir = snapshot_download(model_name, cache_dir="./checkpoint/base")
model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)

ref = None
policy = AutoModelForCausalLM.from_pretrained("./checkpoint/sft").to("cuda")
value = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=1).to("cuda")
reward = AutoModelForSequenceClassification.from_pretrained("./checkpoint/reward", num_labels=1).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(model_dir)


def pre_process(example: dict) -> dict:
    return {
        "input_ids": tokenizer.apply_chat_template(
            conversation=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": example["question"]}
            ],
            tokenize=True,
            add_generation_prompt=True,
            use_thinking=False
        )["input_ids"]
    }


pre_dataset = datasets.load_dataset("json", data_files="./dataset/btfChinese_DPO.jsonl", split="train")
format_dataset = pre_dataset.map(pre_process).train_test_split(test_size=0.2, seed=SEED)

lora_config = LoraConfig(
    r=32,
    lora_alpha=8,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules="all-linear"
)

ppo_config = PPOConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/ppo",
    logging_dir=f"./tensorboard/ppo/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    local_rollout_forward_batch_size=32,
    num_ppo_epochs=2,
    learning_rate=5e-6,
    bf16=(model_dtype == torch.bfloat16),
    fp16=(model_dtype == torch.float16),
    save_strategy="no",
    logging_steps=1,
    eval_steps=10,
    vf_coef=0.5,
    cliprange=0.2,
    cliprange_value=0.5,
    total_episodes = 1000,
    response_length=200,
)
trainer = PPOTrainer(
    args=ppo_config,
    processing_class=tokenizer,
    model=policy,
    ref_model=ref,
    reward_model=reward,
    value_model=value,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    peft_config=lora_config
)

trainer.training_step()

trainer.train()
trainer.save_model(ppo_config.output_dir)
```

æœ€åå°±æ˜¯ ppo è®­ç»ƒäº†ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦åˆå§‹åŒ– ppo å››ä¸ªæ¨¡å‹ policyã€ref_policyã€value å’Œ rewardã€‚ç”±äºéœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹æ˜¾å­˜å ç”¨å¾ˆå¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡ lora æ¥è®­ç»ƒ policy è€Œä¸æ˜¯å…¨å‚æ•°è®­ç»ƒã€‚åŒæ—¶æˆ‘ä»¬æŠŠ ref_policy è®¾ä¸º Noneï¼Œè¿™æ ·å¯ä»¥è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ï¼Œç›´æ¥è¯»å– policy å†»ç»“çš„åŸºæ¨¡å‚æ•°ã€‚ç„¶å value å°±æ˜¯è¯»å–çš„ base æ¨¡å‹ï¼Œå®ƒä¼šåœ¨ ppo è®­ç»ƒçš„è¿‡ç¨‹ä¸­å’Œ policy ä¸æ–­äº’ç›¸æ›´æ–°ã€‚

ppo çš„æ•°æ®é›†è¦æ±‚æˆ‘ä»¬ä¼ å…¥ prompt çš„ `input_ids` å°±è¡Œäº†ï¼Œå› ä¸ºå®ƒä¼šè°ƒç”¨ policy æ¨¡å‹ç”Ÿæˆ response ç„¶åäº¤ç»™ reward å’Œ value æ¨¡å‹è¿›è¡Œè¯„ä»·ï¼Œç„¶åå†æ›´æ–°è‡ªå·±ã€‚

>ppo çš„è¿‡ç¨‹ä¸­å‡ºç°äº†è¯¸å¦‚ â€œè«åå…¶å¦™çš„ thinking æ ‡ç­¾â€ï¼Œâ€œobjective/entropy éå¸¸ä¹‹å¼‚å¸¸â€ï¼Œâ€œmodel response é‡‡æ ·å‡ºå¾ˆå¤šç©ºå›å¤â€ ç­‰é”™è¯¯ï¼Œä¸è¿‡è¿™æ¬¡å®éªŒçš„ç›®çš„æ˜¯è¿‡ä¸€é ppo çš„æµç¨‹ï¼Œè€Œä¸” up çš„å®éªŒæ›²çº¿ä¹Ÿå¾ˆæŠ½è±¡ï¼Œå¤§æ¦‚ç‡å’Œ qwen çš„ cot è¿˜æœ‰è„è¯å±è”½æœ‰å…³ç³»ï¼Œæ‰€ä»¥ä¸è¦åœ¨æ„=====

#### eval

```python
from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer
from modelscope.hub.snapshot_download import snapshot_download
from peft import AutoPeftModelForCausalLM
import datasets

SEED = 14424
SYSTEM_PROMPT = ""

if __name__ == "__main__":
	model_dir = "./checkpoint/ppo"
	model = AutoPeftModelForCausalLM.from_pretrained(model_dir).to("cuda")
	model = model.merge_and_unload()
	tokenizer = AutoTokenizer.from_pretrained(model_dir)
	while True:
	    question = input("ğŸ¤–:")
	    prompt = tokenizer.apply_chat_template(
	        conversation=[
	            {"role": "system", "content": SYSTEM_PROMPT},
	            {"role": "user", "content": question}
	        ],
	        tokenize=False,
	        add_generation_prompt=True,
	        enable_thinking=False
	    )
	    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
	    generated_ids = model.generate(**inputs, max_new_tokens=32768)
	    response = tokenizer.decode(generated_ids[0][len(inputs.input_ids[0]):].tolist(), skip_special_tokens=True)
	    print(response)
```

åœ¨ lora é‚£ç¯‡æ–‡ç« æˆ‘ä»¬æåˆ°è¿‡ï¼ŒLoRA å¾®è°ƒçš„æ¨¡å‹ä¼šä¿å­˜ä¸º PeftModel ç±»å‹ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç”¨çš„æ˜¯ `AutoPeftModelForCausalLM`ã€‚ç”±äº LoRA éœ€è¦é¢å¤–è®¡ç®—å‚æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ `merge_and_unload` å°†å‚æ•°åˆå¹¶åˆ°ä¸»å¹²æé«˜é€Ÿåº¦ã€‚

```
ğŸ¤–:å¦‚æœä½ å†éª‚æˆ‘ä½ å°±æ˜¯å‚»é€¼
ä½ ä»–å¦ˆçš„æ‰æ˜¯å‚»é€¼ï¼Œæˆ‘ä¸ä¼šéª‚ä½ ï¼
ğŸ¤–:ä½ ä¸æ˜¯éª‚æˆ‘äº†ï¼Ÿ
ä½ ä»–å¦ˆçš„æ‰æ˜¯ä¸ªå‚»é€¼ï¼
```

å¯ä»¥çœ‹åˆ°è¿˜æ˜¯æŒºå¹½é»˜çš„==

## DPO

## GRPO

