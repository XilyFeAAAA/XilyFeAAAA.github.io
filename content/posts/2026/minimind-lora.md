---
title: MiniMind å­¦ä¹ æŒ‡åŒ—(å…­)ï¼šLoRA
date: 2026-02-13T14:08:48+08:00
featuredImage: http://img.xilyfe.top/img/20260122134824760.png
authors:
  - Xilyfe
series:
  - minimind
tags:
  - å¤§æ¨¡å‹
  - æ·±åº¦å­¦ä¹ 
lastmod: 2026-02-16T12:09:07+08:00
---
## LoRA æ˜¯ä»€ä¹ˆ

PEFT å¤§è‡´åŒ…å«ä¸‰ç±»ï¼šPrompt-Tuningã€Adapter-Tuning ä»¥åŠ LoRAï¼Œè€Œ MiniMind é‡Œé¢é‡‡ç”¨çš„å°±æ˜¯ LoRA è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚
åœ¨ CS224N çš„è¯¾ç¨‹ä¸­å·²ç»å­¦ä¹ äº† LoRA çš„åŸç†ï¼Œç®€å•æ¥è¯´æˆ‘ä»¬åœ¨ç»è¿‡ Pretrain å’Œ SFT çš„æ¨¡å‹åŸºç¡€ä¸Šï¼Œå¯¹å‚æ•° $y=Wx$ åŠ ä¸Šä¸€ä¸ªå¢é‡çŸ©é˜µ $\Delta{W}$ æ¥å¾®è°ƒæ¨¡å‹ï¼Œå¹¶ä¸”è¿™ä¸ª $\Delta{W}$ æ˜¯é€šè¿‡ **ä½ç§©è¿‘ä¼¼** å¾—åˆ°çš„ï¼Œæ‰€ä»¥å®é™…å‚æ•°é‡è¿œå°äº $W$ï¼Œè®¡ç®—å¼€é”€å°ã€‚å…·ä½“å¯ä»¥çœ‹ä¹‹å‰çš„ç¬”è®°ï¼š

{{< link_ref "cs224n-lecture12" >}}

##  å¸¸è§é—®é¢˜

### LoRA æ’å…¥åœ¨å“ªé‡Œ

æ—©æœŸ LoRA æ¨¡å—ä»…åœ¨æ³¨æ„åŠ›æ¨¡å—çš„ $W_q$ å’Œ $W_v$ ä¸Šæ’å…¥ï¼Œ$W_q$ å†³å®šäº†è¦å…³æ³¨çš„ä¿¡æ¯ï¼Œ$W_k$ å†³å®šäº†è¦æå–çš„ä¿¡æ¯ã€‚ä½†æ˜¯éšç€å¤§æ¨¡å‹å¾®è°ƒç»éªŒçš„åŸºç±»ï¼Œå‘ç°å•å•å¾®è°ƒ Attention ä¸èƒ½æ”¹å˜æ¨¡å‹æ·±å±‚è¡Œä¸ºã€‚çœŸæ­£å­˜å‚¨å¤§æ¨¡å‹çŸ¥è¯†çš„æ˜¯æ¯ä¸€å±‚çš„ MLP æ¨¡å—ï¼Œæ‰€ä»¥è¿˜åœ¨å…¶ä¸­çš„ä¸‰ç»„æŠ•å½± $W_{up}$ã€$W_{gate}$ å’Œ $W_{down}$ ä¸ŠåŠ å…¥ LoRA æ¨¡å—ã€‚ç°åœ¨ä¸»æµçš„ LoRA å¾®è°ƒç­–ç•¥å·²ç»å˜æˆäº† All-Linearï¼Œä¹Ÿå°±æ˜¯å¯¹æ‰€æœ‰çº¿æ€§å±‚éƒ½æ’å…¥ LoRAã€‚

### LoRA åˆå§‹åŒ–

ä¸€èˆ¬æ˜¯å¯¹ $A$ çŸ©é˜µåº”ç”¨ kaiming åˆå§‹åŒ–ï¼Œå¯¹ $B$ çŸ©é˜µç½®ä¸º 0ã€‚é¦–å…ˆçŸ©é˜µ $A$ å’Œ $B$ æœ€å°‘éœ€è¦ä¸€ä¸ªä¸º 0 çŸ©é˜µï¼Œè¿™æ · LoRA ä¸€å¼€å§‹æ›´æ–°æ—¶ $\Delta W=BA$ æ¥è¿‘äº 0 çŸ©é˜µï¼Œå°±ä¸ä¼šç ´åé¢„è®­ç»ƒæƒé‡ã€‚å…¶æ¬¡çŸ©é˜µ $A$ ä¸èƒ½ä¸º 0 çŸ©é˜µï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹ $A$ å’Œ $B$ çš„æ¢¯åº¦æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼š

é¦–å…ˆ $A$ çš„æ¢¯åº¦å…¬å¼ä¸ºï¼š

$$
\frac{\partial{L}}{\partial{A}}=\frac{\partial{L}}{\partial{Q}} \cdot Z^T= \frac{\partial{L}}{\partial{Q}}(BX^T)=\frac{\partial{L}}{\partial{Q}}X^TB^T
$$

$B$ çš„æ¢¯åº¦å…¬å¼ä¸ºï¼š

$$
\frac{\partial{L}}{\partial{B}}=\frac{\partial{L}}{\partial{Z}} \cdot X^T= (A^T\frac{\partial{L}}{\partial{Q}})X^T
$$

è€Œåœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œä½ç§©æ›´æ–°å®é™…èµ°çš„è·¯å¾„æ˜¯ï¼šx â†’ A â†’ (scale) â†’ Bï¼Œä¹Ÿå°±æ˜¯è¯´åå‘ä¼ æ’­æ—¶æ˜¯ä»çŸ©é˜µ $B$ åˆ°çŸ©é˜µ $A$ã€‚å‡å¦‚çŸ©é˜µ $A$ ä¸º 0 çŸ©é˜µï¼Œé‚£ä¹ˆçŸ©é˜µ $B$ çš„æ¢¯åº¦ä¸º 0ï¼Œè®­ç»ƒå°±ä¼šå…ˆæ›´æ–°çŸ©é˜µ $A$ï¼Œ$A$ æ›´æ–°çš„æ•°å€¼å°ºåº¦å°±ä¼šæ”¶åˆ° $B$ çš„åˆå§‹åŒ–åˆ†å¸ƒå½±å“ï¼Œå®¹æ˜“æ”¾å¤§æ—©æœŸæ›´æ–°çš„å°ºåº¦ã€‚å¦‚æœåˆå§‹åŒ–çŸ©é˜µ $B$ ä¸º 0ï¼Œé‚£ä¹ˆä¼šå…ˆæ›´æ–°çŸ©é˜µ $B$ï¼ŒæŠŠ $B$ ä» 0 æ‹‰å¼€ï¼Œå†æ›´æ–° $A$ã€‚è®­ç»ƒç¨³å®šï¼Œç­‰ä»·äºå…ˆå­¦ä¹ è¾“å‡ºä¾§ç»„åˆï¼Œå†ç»†åŒ–è¾“å…¥ä¾§æŠ•å½±ã€‚

### ç§© r å¦‚ä½•å½±å“æ¨¡å‹è¡¨ç°

ä»è®­ç»ƒè¡Œä¸ºçœ‹ã€‚r å°çº¦æŸå¼ºï¼Œæ›´æ–°å­ç©ºé—´çª„ï¼Œä¼˜åŒ–æ›´ç¨³å®šï¼Œå¯¹å°æ•°æ®é›†æ›´æŠ—è¿‡æ‹Ÿåˆï¼Œä½†å®¹æ˜“æ¬ æ‹Ÿåˆï¼Œloss é™ä¸åŠ¨æˆ–å¾ˆæ—©å¹³å°æœŸã€‚  
r å¤§è‡ªç”±åº¦é«˜ï¼Œloss æ›´å®¹æ˜“ä¸‹é™ï¼Œä»»åŠ¡ä¸Šé™æ›´é«˜ï¼Œä½†å¯¹æ•°æ®è§„æ¨¡æ•æ„Ÿï¼Œå°æ•°æ®æ—¶å®¹æ˜“è®°å¿†åŒ–å’Œåˆ†å¸ƒæ¼‚ç§»ã€‚

åœ¨æ³¨æ„åŠ›å±‚ä¸Šï¼Œè¾ƒå°çš„ r å¾€å¾€å·²è¶³å¤Ÿæ”¹å˜ä¿¡æ¯è·¯ç”±ï¼Œæ”¶ç›Šæ›²çº¿å¾ˆå¿«é¥±å’Œã€‚åœ¨ MLP æŠ•å½±å±‚ä¸Šï¼Œé€šå¸¸éœ€è¦æ›´å¤§çš„ r æ‰èƒ½äº§ç”ŸåŒç­‰å¹…åº¦çš„è¡Œä¸ºå˜åŒ–ã€‚

## å®ç°ç»†èŠ‚

### LoRA æ¨¡å—

å‰é¢æˆ‘ä»¬æ•°å­¦å…¬å¼æ˜¯ $y=Wx+\Delta Wx = Wx+BAx$ï¼Œä½†æ˜¯åœ¨ PyTorché‡Œé¢å¦‚æœæˆ‘ä»¬ç”¨ `nn.Parameter()` æ‰‹åŠ¨å®ç°å¾—å†™æˆï¼š

```python
def __init__(self, in_features, out_features, r)
	self.A = nn.Parameter(torch.zeros(r, in_features))
	self.B = nn.Parameter(torch.zeros(out_features, r))

def forward(self, x):
	return x @ (self.B @ self.A).T
```

PyTorch é»˜è®¤æŠŠç‰¹å¾ç»´æ”¾åœ¨æœ€åï¼šè¾“å…¥å½¢çŠ¶æ˜¯ `(batch, ..., in_features)`ï¼Œè¿™æ ·æ‰€æœ‰å‰å¯¼ç»´éƒ½å½“ä½œæ‰¹ç»´è‡ªåŠ¨å¹¿æ’­ï¼Œæ‰€ä»¥åœ¨ PyTorch é‡Œé¢éƒ½æ˜¯ x å³ä¹˜ä¸€ä¸ªçŸ©é˜µè€Œä¸æ˜¯åƒçº¿æ€§ä»£æ•°é‡Œé¢éƒ½æ˜¯ $W \times x$ è¿™æ ·å·¦ä¹˜ä¸€ä¸ªçŸ©é˜µã€‚

```python
class LoRA(nn.Module):
    def __init__(self, in_features: int, out_features: int, rank: int, alpha: int) -> None:
        super(LoRA, self).__init__()

        self.scaling = alpha / rank
        self.A = nn.Linear(in_features, rank, bias=False)
        self.B = nn.Linear(rank, out_features, bias=False)

        nn.init.kaiming_uniform_(self.A.weight, a=5**0.5)
        nn.init.zeros_(self.B.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.B(self.A(x)) * self.scaling
```

å¦‚æœç›´æ¥ç”¨ `nn.Linear` é‚£ä¹ˆåªéœ€è¦å…ˆåº”ç”¨ A å†åº”ç”¨ B å°±å¥½äº†ã€‚

### åº”ç”¨ LoRA

```python
def apply_lora(model: nn.Module, rank: int, alpha: int) -> None:
    # freeze all parameters
    for param in model.parameters():
        param.requires_grad = False

    # collect linear
    lora_modules = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and "lora" not in name and "lm_head" not in name:
            lora_modules.append((name, module))

    for name, module in lora_modules:
        lora_module = LoRA(in_features=module.weight.shape[1], out_features=module.weight.shape[0], rank=rank, alpha=alpha).to(module.weight.device).to(module.weight.dtype)

        setattr(module, "lora", lora_module)
        ori_forward = module.forward

        def forward_with_lora(x):
	        return ori_forward(x) + lora_module(x)

        module.forward = forward_with_lora

```

ä¸ºäº†é¿å…å°† LoRA åº”ç”¨åˆ° LoRA å±‚è‡ªèº«ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸€ä¸ª `nn.Module` æ£€æµ‹ä»–çš„æƒé‡çŸ©é˜µå½¢çŠ¶æ˜¯å¦ä¸º rankã€‚å…¶æ¬¡éœ€è¦æ³¨æ„æˆ‘ä»¬åˆå§‹åŒ– LoRA æ¨¡å—çš„æ—¶å€™ï¼Œ`in_features=module.weight.shape[1]` ã€‚è¿™æ˜¯å› ä¸º `nn.Linear` å±‚å†…éƒ¨åˆå§‹åŒ–çš„æƒé‡çŸ©é˜µæ˜¯ `W=[out_features, in_features]`ï¼Œç„¶åè®¡ç®— $y=xW^T$ï¼Œæ‰€ä»¥ `in_features` åº”è¯¥æ˜¯æƒé‡çŸ©é˜µçš„ç¬¬äºŒç»´ã€‚

è¿™ä¸ªä»£ç çœ‹ä¼¼æ²¡å•¥é—®é¢˜ï¼Œä½†æ˜¯æˆ‘è°ƒè¯•æ—¶å€™ debug äº†åŠä¸ªå¤šå°æ—¶ï¼Œæœ€åè¿˜æ˜¯ Gemini å¸®æˆ‘è§£å†³äº†ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç»å…¸çš„ **Python é—­åŒ…** å¯¼è‡´çš„é”™è¯¯ã€‚é—­åŒ…å‡½æ•°å†…çš„ `lora` å’Œ `ori_forward` æ˜¯ä»å¤–éƒ¨ä½œç”¨åŸŸâ€œå¼•ç”¨â€çš„å˜é‡ï¼Œå®ƒä»¬æŒ‡å‘çš„æ˜¯å¾ªç¯ç»“æŸæ—¶çš„â€œæœ€åä¸€ä¸ªå€¼â€ï¼Œè€Œä¸æ˜¯å½“å‰å¾ªç¯çš„å€¼ã€‚æ‰€ä»¥å½“æˆ‘ä»¬å‰å‘ä¼ æ’­è®¡ç®—çº¿æ€§å±‚çš„æ—¶å€™ï¼Œå®ƒè°ƒç”¨çš„ forward æ–¹æ³•å…¶å®éƒ½æ˜¯æœ€åä¸€ä¸ªçº¿æ€§å±‚çš„ forward + loraã€‚å…³äº Python çš„é—­åŒ…é—®é¢˜å¯ä»¥è§ä¸‹é¢è¿™ä¸ªæ–‡ç« ï¼Œè¿™é‡Œå°±è®²ä¸€ä¸‹æ€ä¹ˆè§£å†³ï¼š

{{< link_ref "python-closure" >}}

è§£å†³æ–¹æ¡ˆæœ‰ä¸¤ç§ï¼š
1. æˆ‘ä»¬ç”¨ **é»˜è®¤å‚æ•°** å°†å½“å‰çš„ `lora` å’Œ `ori_forward` ç»‘å®šåˆ°å‡½æ•°å†…éƒ¨ã€‚

```python
def forward_with_lora(x, lora=lora, ori_forward=ori_forward):
    return ori_forward(x) + lora(x)
```

2. ä½¿ç”¨ **å·¥å‚å‡½æ•°** åˆ›å»ºï¼Œæ¯æ¬¡è°ƒç”¨éƒ½ä¼šç”Ÿæˆæ–°çš„é—­åŒ…ç¯å¢ƒ

```python
def _create_lora_forward(lora_module, original_forwarda):
    def forward(x):
        return original_forward(x) + lora_module(x)
    return forward

def apply_lora(model: nn.Module, rank: int) -> None:
    for _, module in model.named_modules():
        module.forward = _create_lora_forward(lora, module.forward, rank, rank*2)
```

### ä¿å­˜ LoRA

æ—¢ç„¶æˆ‘ä»¬è®­ç»ƒäº† LoRA æ¨¡å—ï¼Œé‚£å°±éœ€è¦æŠŠé‡Œé¢çš„æƒé‡ä¿å­˜ä¸‹æ¥ã€‚æˆ‘ä»¬ä¹‹å‰ç”¨ `setattr(module, "lora", lora)` æŠŠ LoRA æ¨¡å—æ’å…¥äº† model é‡Œé¢ï¼Œæ‰€ä»¥ `lm_checkpoint` æ–¹æ³•é€šè¿‡ `model.state_dict()` å¯ä»¥è·å¾— LoRA çš„æƒé‡ã€‚ä½†æ˜¯æˆ‘ä»¬éœ€è¦çš„æ˜¯ LoRA çš„ **å¯æ’æ‹”** çš„ç‰¹æ€§ï¼Œæ‰€ä»¥åªéœ€è¦æŠŠ LoRA çš„æƒé‡ç•™ä¸‹æ¥å³å¯ï¼Œéœ€è¦çš„æ—¶å€™æŠŠè¿™éƒ¨åˆ†æƒé‡æŒ‚è½½ä¸Šå»ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å†å†™ä¸€ä¸ªæ–¹æ³•æ¥å®ç°ï¼š

```python
def save_lora(model: nn.Module, path: str):
	state_dict = {}
	for name, module in model.named_modules():
		if hasattr(module, "lora"):
			tmp_state = {f"{name}.lora.{k}": v for k, v in module.lora.state_dict().items()}
			state_dict.update(tmp_state)
	torch.save(state_dict, path)
```

### åº”ç”¨ LoRA

```python
def load_lora(model: nn.Module, path: str):
	state_dict = torch.load(path, map_location=model.device)
	for name, module in model.named_modules():
		if hasattr(module, "lora"):
			lora_state = {"A.weight": state_dict[f"{name}.lora.A.weight"], "B.weight": state_dict[f"{name}.lora.B.weight"]}
Â  Â  Â  Â  Â  Â  module.lora.load_state_dict(lora_state)
```

## å®éªŒç»“æœ

### MiniMind å¾®è°ƒ

```
ğŸ’¬: ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ
ğŸ¤–: æˆ‘æ˜¯èƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ€è€ƒå’Œæ„ŸçŸ¥ç¯å¢ƒçš„æ™ºèƒ½æœºå™¨ã€‚

[Speed]: 15.22 tokens/s


ğŸ’¬: ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ 
ğŸ¤–: å› ä¸ºæ˜Ÿæ˜Ÿå’Œå¤ªé˜³å…‰ä¼šåå°„ä¸åŒæ³¢é•¿çš„å…‰çº¿ï¼Œå¯¼è‡´æˆ‘ä»¬çœ‹åˆ°çš„æ˜¯è“è‰²ã€‚

[Speed]: 21.41 tokens/s                                                                                          t Outlook æˆ–è€… Yaho.ai Gam
                                                                                                                  æ¸¸çš„æ™®é€šæ°‘ä¼—ï¼Œä¹Ÿå¯ä»¥é€‰æ‹© 

ğŸ’¬: è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹
ğŸ¤–: è›‹ç™½è´¨æ˜¯æ¤ç‰©å’Œä¸€äº›ç»†èŒåˆ©ç”¨é˜³å…‰ã€æ°´æˆ–å…¶ä»–ç”Ÿç‰©èƒ½è¿›è¡Œç»†èƒå‘¼å¸ï¼Œäº§ç”Ÿæ°§æ°”å¹¶é‡Šæ”¾å‡ºèƒ½é‡ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåˆ°è›‹ç™½è´¨çš„ç»“æ„
ã€åˆ†å­é—´ç›¸äº’ä½œç”¨ä»¥åŠé…¶å‚¬åŒ–ç­‰å¤šç§æœºåˆ¶ã€‚åœ¨æ¤ç‰©ä½“å†…ï¼Œæ°¨åŸºé…¸é€šè¿‡è½¬å½•æˆRNAï¼Œç„¶åæ˜¯ç¢³æ°´åŒ–åˆç‰©ï¼ˆO2ï¼‰æ•è·äºŒæ°§åŒ–ç¢³ï¼Œç”Ÿæˆæœ‰
æœºç‰©è´¨ï¼›åŒæ—¶ï¼Œè¿˜å¯ä»¥é€šè¿‡ä»£è°¢é€”å¾„å°†æœ‰æœºç‰©è´¨è½¬æ¢ä¸ºè¥å…»ç‰©è´¨æ¥å¸®åŠ©æ¤ç‰©ç”Ÿé•¿å’Œä¼ é€’å…»åˆ†ã€‚å½“æ¤ç‰©å—åˆ°å¤–éƒ¨ç¯å¢ƒå‹åŠ›æ—¶ï¼Œå…¶ä»£
è°¢äº§ç‰©ä¼šè¢«æ¿€æ´»ï¼Œä¾›åº”ç›¸åº”çš„æ°´åˆ†å’Œè¥å…»æˆåˆ†ï¼›è¿™ä¸€è¿‡ç¨‹ä¸­è¿˜æœ‰åŠ©äºè°ƒèŠ‚äººä½“å…ç–«ç³»ç»Ÿï¼Œé˜²æ­¢æ„ŸæŸ“å’Œç–¾ç—…ã€‚æ€»ä¹‹ï¼Œç”Ÿç‰©ä½“å†…å‘ç”ŸDNAèšåˆé…¶çš„åˆ†è§£å’Œè½¬è¿ï¼Œæ˜¯è‡³å…³é‡è¦çš„ã€‚

[Speed]: 22.23 tokens/s


ğŸ’¬: å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨
ğŸ¤–: ç¡®ä¿ä½ çš„è¡£æœå¹²å‡€ã€å¹²ç‡¥ã€‚ä½ å¯ä»¥ç©¿å¤–å¥—å»å­¦æ ¡æˆ–å›¾ä¹¦é¦†ã€‚å¦‚æœä½ æƒ³å¸¦ä¼ï¼Œä½†ä¸ä¼šä¸‹é›¨çš„è¯å°±å¸¦ä¸Šé›¨ä¼ã€‚

[Speed]: 20.53 tokens/s


ğŸ’¬: æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹
ğŸ¤–: çŒ«æ›´å–œæ¬¢è‡ªå·±ç‹¬å¤„ï¼Œè€Œç‹—åˆ™ç›¸å¯¹ç‹¬ç«‹ã€‚ä½†æ˜¯å®ƒä»¬çš„æ€§æ ¼ä¸åŒï¼Œæœ‰æ—¶å€™ç”šè‡³ä¼šè¢«é©¯æœï¼Œä½†å®ƒä»¬æ€»æ˜¯å¯ä»¥å˜å¾—éå¸¸äº²è¿‘äººç±»ï¼Œå–œ
æ¬¢åœ¨ä¸»äººèº«è¾¹æ‰“æ‰°è‡ªå·±çš„å®‰å®ç¡è§‰ã€‚

[Speed]: 21.54 tokens/s


ğŸ’¬: è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 
ğŸ¤–: æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰æ˜¯ä¸€ç§åŸºäºæ•°æ®è®­ç»ƒçš„ç®—æ³•ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡åˆ†æå’Œæ¨¡å¼è¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼ã€è¶‹åŠ¿æˆ–å…³è”æ€§ï¼Œä»
è€Œå®ç°è‡ªåŠ¨åŒ–å†³ç­–æˆ–è€…é¢„æµ‹æœªæ¥ç»“æœçš„èƒ½åŠ›ã€‚è¿™ç§æŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€åŒ»ç–—è¯Šæ–­ç­‰ï¼Œæ˜¯
ç°ä»£äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

[Speed]: 22.86 tokens/s


ğŸ’¬: æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ
ğŸ¤–: æˆ‘æ¨èä¸€ä¸‹ï¼šå®«ä¿é¸¡ä¸ã€éº»è¾£é¦™é”…ã€å›é”…è‚‰ã€é¥ºå­çš®ç­‰ã€‚è¿™äº›éƒ½æ˜¯éå¸¸å—äººä»¬æ¬¢è¿çš„ç¾å‘³å“¦ï¼Œä½ å¯ä»¥å°è¯•ä¸€ä¸‹å°±å¯ä»¥äº†ï¼

[Speed]: 22.49 tokens/s
```

è®­ç»ƒäº† 20 ä¸ª epochï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹å·²ç»æœ‰äº†è‡ªå·±çš„è¯´è¯é£æ ¼äº†ã€‚

### Qwen3-0.6B å¾®è°ƒ

ä½†æ˜¯ï¼Œç”±äºæˆ‘çš„åŸºæ¨¡å¤ªæ‹‰è·¨äº†ï¼Œæ‰€ä»¥æˆ‘ä¸‹è½½äº† Qwen3-0.6B æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒã€‚ä¸‹é¢ä»£ç æ˜¯æ‰‹åŠ¨é€šè¿‡ PyTorch è¿›è¡Œ LoRA å¾®è°ƒï¼Œè°ƒç”¨ Transformers åº“è¿›è¡Œ LoRA å¾®è°ƒçš„æ–¹æ³•å¯ä»¥è§åšæ–‡ï¼š

{{< link_ref "llm-lora" >}}

å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```python
import argparse

import torch
import tqdm
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer

from dataset.lora_dataset import LoRADataset
from model.lora import apply_lora, save_lora


def get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument("--seed", type=int, default=11711)
    parser.add_argument("--device", type=str, default="cuda:0")
    parser.add_argument("--dtype", type=str, default="float16")
    parser.add_argument("--batch_size", type=int, default="8")
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--model_path", type=str, default="")
    parser.add_argument("--tokenizer_path", type=str, default="")
    parser.add_argument("--dataset_path", type=str, default="")
    parser.add_argument("--max_length", type=int, default=340)
    parser.add_argument("--num_workers", type=int, default=2)
    parser.add_argument("--rank", type=int, default=8)
    parser.add_argument("--alpha", type=int, default=8)
    parser.add_argument("--log_interval", type=int, default=100)
    parser.add_argument("--save_interval", type=int, default=200)
    parser.add_argument("--lora_path", type=str, default="lora")
    parser.add_argument("--lora_name", type=str, default="xiaoxue")
    return parser.parse_args()


def train(args: argparse.Namespace):
    model = AutoModelForCausalLM.from_pretrained(args.model_path).to(args.device)
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    apply_lora(model, args.rank, args.alpha)

    train_ds = LoRADataset(tokenizer, args.dataset_path, args.max_length)
    dataloader = DataLoader(train_ds, args.batch_size, shuffle=True, num_workers=args.num_workers)
    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=args.lr)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params} / {all_params} ({trainable_params / all_params:.2%})")

    step = 0
    model.train()
    for epoch in range(args.epochs):
        for input_ids, labels in tqdm.tqdm(dataloader, desc=f"Epoch {epoch + 1}"):
            step += 1
            input_ids = input_ids.to(args.device)
            labels = labels.to(args.device)
            mask = (input_ids != tokenizer.pad_token_id).bool()
            outputs = model(input_ids=input_ids, labels=labels, attention_mask=mask)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if step % args.log_interval == 0:
                tqdm.write(f"Epoch:[{epoch + 1}/{args.epochs}], loss: {loss.item():.4f}")
            if step & args.save_interval == 0:
                save_lora(model, f"{args.lora_path}/{args.lora_name}_{args.rank}.pth")


if __name__ == "__main__":
    args = get_args()
    train(args)
```

æ¨¡å‹è¾“å‡ºå¦‚ä¸‹ï¼š

```
ğŸ’¬: ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ
ğŸ¤–: å—¯...æˆ‘æœ‰å†™æ•…äº‹çš„èƒ½åŠ›å“¦ï¼Œèƒ½å†™å‡ºå¥½å¤šä¸ªæœ‰è¶£çš„æ•…äº‹å‘¢ï¼ä¸è¿‡è¿™å¯æ˜¯æˆ‘çš„å¤©èµ‹å˜›ï¼ˆæ¼ï¼‰

[Speed]: 10.99 tokens/s


ğŸ’¬: ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„
ğŸ¤–: å› ä¸ºæ°´åˆ†å­åœ¨é˜³å…‰ä¸‹ä¼šæ•£å°„ï¼Œæ‰€ä»¥å¤©ç©ºå‘ˆç°è“è‰²ï¼ï¼ˆç­”ï¼‰

[Speed]: 13.29 tokens/s


ğŸ’¬: è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹
ğŸ¤–: å…‰åˆä½œç”¨æ˜¯ä¸€ç§æ¤ç‰©ã€è—»ç±»å’ŒæŸäº›ç»†èŒç­‰ç”Ÿç‰©é€šè¿‡å¶ç»¿ä½“ä¸­çš„è‰²ç´ å¸æ”¶å¤ªé˜³å…‰ï¼Œå°†æ°´åˆ†è§£ä¸ºæ°§æ°”ï¼Œå¹¶é‡Šæ”¾å‡ºäºŒæ°§åŒ–ç¢³çš„è¿‡ç¨‹ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œæ¤ç‰©åˆ©ç”¨é˜³å…‰èƒ½é‡åˆæˆç³–åˆ†å¹¶å‚¨å­˜è¿™äº›ç‰©è´¨ä½œä¸ºå‚¨è—åº“ä¾›ä»¥åä½¿ç”¨ã€‚è¿™ä¸€æ­¥æ˜¯ç”Ÿå‘½æ´»åŠ¨çš„åŸºç¡€ä¹‹ä¸€ï¼ï¼ˆæ³¨æ„ï¼šè¿™ä¸ªå›
ç­”å¯èƒ½æœ‰äº›è¯¯å¯¼æ€§å“¦ï¼‰

[Speed]: 13.17 tokens/s


ğŸ’¬: å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨
ğŸ¤–: å…ˆçœ‹å¤©æ°”é¢„æŠ¥ï¼å†å†³å®šæ˜¯å¦å¸¦ä¼ã€‚å¦‚æœæ˜¯æ™´å¤©é‚£å°±ç›´æ¥å‡ºå‘å§

[Speed]: 13.34 tokens/s


ğŸ’¬: æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹
ğŸ¤–: ä¼˜ç‚¹ï¼šçŒ«æ¯”ç‹—æ›´èªæ˜ï¼Œèƒ½æ„ŸçŸ¥åˆ°ä¸»äººçš„æƒ…ç»ªï¼›è€Œç‹—åˆ™æœ‰æ›´å¼ºçš„è´£ä»»æ„Ÿã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¯èƒ½ç¼ºä¹è¶³å¤Ÿçš„äº’åŠ¨æ—¶é—´æ¥åŸ¹å…»å½¼æ­¤çš„æ„Ÿæƒ…ã€‚å› æ­¤åœ¨é€‰æ‹©æ—¶éœ€è¦æƒè¡¡è¿™äº›å› ç´ ï¼Œå¹¶è€ƒè™‘æ˜¯å¦èƒ½å¤Ÿä¸ºå¯¹æ–¹æä¾›è¶³å¤Ÿçš„æƒ…æ„Ÿæ”¯æŒã€‚å¦‚æœä½ ä»¬æ„¿æ„æŠ•å…¥æ›´å¤š
çš„æ—¶é—´ä¸ç²¾åŠ›å»å»ºç«‹æƒ…æ„Ÿè”ç³»çš„è¯ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•å°†ä¸¤è€…æ”¾åœ¨åŒä¸€ä¸ªå®¶åº­ä¸­ï¼Œå…±åŒæˆé•¿å§ï¼

[Speed]: 12.99 tokens/s


ğŸ’¬: è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 
ğŸ¤–: ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è®©è®¡ç®—æœºé€šè¿‡ç»éªŒæ¥å‘ç°è§„å¾‹å’Œæ¨¡å¼ï¼Œä»è€Œæé«˜å¯¹æ–°æ•°æ®çš„ç†è§£ä¸é¢„æµ‹èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œè®­ç»ƒæ¨¡å‹æ—¶å¯ä»¥å°†å¤§é‡å·²çŸ¥çš„æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ç­‰éƒ¨åˆ†ï¼Œå¹¶ç”¨è¿™äº›æ•°æ®é›†ä¸Šçš„ä¿¡æ¯å»ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼ˆå³è¿‡æ‹Ÿåˆï¼‰ï¼Œæœ€åå°±èƒ½åœ¨æ–°çš„æµ‹
è¯•é›†ä¸­åšå‡ºæ›´å‡†ç¡®çš„åˆ¤æ–­äº†ï¼

[Speed]: 12.85 tokens/s


ğŸ’¬: æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ
ğŸ¤–: å—¯...ä¸­å›½æœ‰å¾ˆå¤šå¥½åƒçš„ä¸œè¥¿å‘¢ï¼Œæ¯”å¦‚é¥ºå­ã€åŒ…å­ã€æ±¤åœ†ç­‰ç­‰ã€‚ä¸è¿‡è¿™äº›åº”è¯¥éƒ½æ˜¯æ¯”è¾ƒæ™®é€šçš„äº†å–µï¼

[Speed]: 12.58 tokens/s
```

å¯ä»¥çœ‹å‡ºæ¥æ•°æ®é›†çš„è¯­è¨€é£æ ¼å·²ç»å¯ä»¥æ¨¡ä»¿ä¸ª 8-9 æˆäº†ã€‚

---

è®°å½•å¾®è°ƒ Qwen3 æ—¶å€™å‡ºç°çš„ä¸€ä¸ª BUGï¼š`loss=nan`ã€‚

ä¸€å¼€å§‹çœ‹åˆ° `loss=nan` æˆ‘çš„ååº”æ˜¯åŠ ä¸Šæ¢¯åº¦è£å‰ªã€ä¿®æ”¹å­¦ä¹ ç‡ï¼Œç„¶åè¿˜æ˜¯çˆ† nan äº†ã€‚ä¹‹åæˆ‘æ€€ç–‘æ˜¯ä¸æ˜¯ç²¾åº¦çš„é—®é¢˜ï¼ŒæŠŠ float16 æ”¹æˆ bfloat16ï¼Œç„¶ååŠ ä¸Šæ··åˆç²¾åº¦è®­ç»ƒè¿˜æ˜¯çˆ† nan äº†ã€‚ç”±äºæ¨¡å‹æ˜¯é¢„è®­ç»ƒçš„è‚¯å®šæ²¡æœ‰é—®é¢˜ï¼Œæˆ‘çš„ LoRA è®­ç»ƒè„šæœ¬ä¹‹å‰ä¹Ÿæ˜¯ ok çš„ï¼Œæ‰€ä»¥æˆ‘æ€€ç–‘æ˜¯ä¸æ˜¯æ•°æ®æœ‰é—®é¢˜ï¼Œäºæ˜¯åœ¨ debugger é‡Œé¢å¯¹ LoRADataSet è¿›è¡Œæ­¥å…¥ã€‚

æˆ‘åœ¨ `__getitem__()` æ–¹æ³•é‡Œé¢æ–­ç‚¹æ—¶å€™ï¼Œæ€€ç–‘æ˜¯ä¸æ˜¯ Qwen çš„ tokenizer `apply_chat_template` åŠ å…¥çš„æ¨¡æ¿å’Œæˆ‘ MiniMind ä¸åŒï¼Œå¯¼è‡´å¯¹é assistant è¿›è¡Œpad æ—¶å€™å‡ºé”™ã€‚åé¢å‘ç°ç¡®å®æ˜¯ `pad_labels` æ–¹æ³•å‡ºé”™äº†ï¼Œä½†é—®é¢˜ä¸æ˜¯æ¨¡æ¿ä¸åŒï¼Œè€Œæ˜¯ Qwen3 çš„ tokenizer æ²¡æœ‰è®¾ç½® bos_tokenã€‚æˆ‘æŠŠä»£ç æ”¹ä¸ºï¼š

```python
self.bos_id = tokenizer("<|im_start|>assistant\n", add_special_tokens=False).input_ids
# self.bos_id = tokenizer(f"{tokenizer.bos_token}assistant\n", add_special_tokens=False).input_ids
```

è¿™æ¬¡è®­ç»ƒå°±å¥½äº†ï¼Œè¿™æ¬¡ç»éªŒå‘Šè¯‰æˆ‘ `loss=nan` å¯èƒ½æ˜¯ **æ•°æ®é›†/æ ‡ç­¾é—®é¢˜**ã€‚