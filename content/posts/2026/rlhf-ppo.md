---
title: LLM ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼šPPO
date: 2026-02-19T15:38:09+08:00
featuredImage: http://img.xilyfe.top/img/20260226203659803.png
authors:
  - Xilyfe
series:
  - RLHF
tags:
  - å¤§æ¨¡å‹
  - å¼ºåŒ–å­¦ä¹ 
lastmod: 2026-02-27T12:57:57+08:00
---
## å‰ç½®çŸ¥è¯†

### è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–

PPO ç®—æ³•å¯ä»¥çœ‹æˆ A2C çš„ä¼˜åŒ–ç‰ˆã€‚A2Cçš„è®­ç»ƒç­–ç•¥æ˜¯ â€œé‡‡æ ·ä¸€æ¬¡ï¼Œæ›´æ–°ä¸€æ¬¡ï¼Œç„¶åæ‰”æ‰æ•°æ®â€ï¼Œè¿™å°±å¯¼è‡´æ•ˆç‡å¾ˆä½ï¼Œæ¯æ‰¹æ•°æ®åªèƒ½ç”¨ä¸€æ¬¡ã€‚PPO é‡‡ç”¨ **è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**ï¼Œå®ƒä¼šå¯¹ä¸€æ‰¹æ•°æ®å…ˆè¿›è¡Œé‡‡æ ·å¾—åˆ° `logprob`ï¼Œ`ref_logprob`ï¼Œ`rewards`ï¼Œ`advantages` ç­‰æ•°æ®ï¼Œç„¶åè¿›è¡Œ `ppo_epochs` æ¬¡å¾ªç¯ã€‚æ¯æ¬¡å¾ªç¯å†…ï¼Œå˜åŒ–çš„åªæœ‰æ¦‚ç‡åˆ†å¸ƒ `logprob` å’Œ `ref_logprob` å’Œ `values`ï¼Œä¼˜åŠ¿å¥–åŠ±è¿™äº›éƒ½å›ºå®šé‡‡ç”¨ç¬¬ä¸€æ¬¡å¾—åˆ°çš„æ•°æ®ã€‚

>ä¸¾ä¸ªä¾‹å­ï¼šA2C å°±åƒåœ¨è¡¨æ¼”ç°åœºï¼Œä½ ä¸€è¾¹æ¼”ï¼Œå¯¼æ¼”ä¸€è¾¹å–Šâ€œå¥½â€æˆ–â€œåâ€ï¼Œç„¶åä½ å¾—åˆ°åé¦ˆå°±ä¿®æ”¹ã€‚æ”¹å®Œä¹‹åï¼Œåˆšæ‰æ¼”çš„é‚£æ®µæˆå°±æ²¡ç”¨äº†ï¼Œä½ å¿…é¡»é‡æ–°æ¼”ä¸€æ®µï¼Œå¯¼æ¼”æ‰èƒ½ç»™æ–°åé¦ˆã€‚è€Œ PPO æ›´åƒ **å¤ç›˜å½•åƒ**ï¼Œä½ å…ˆæ¼”ä¸€æ®µæˆå½•ä¸‹æ¥ï¼Œæ¥ä¸‹æ¥çš„ 4 ä¸ª Epoch ä½ ååœ¨ç›‘è§†å™¨å‰ï¼Œå¯¹ç€è¿™æ®µå½•åƒåå¤ç¢ç£¨ã€‚ç¬¬ä¸€éæ ¹æ®åé¦ˆæ”¹ä¸€ç‚¹ï¼Œç¬¬äºŒéåœ¨ç¬¬ä¸€éæ”¹åŠ¨çš„åŸºç¡€ä¸Šï¼Œå†å¯¹ç€å½•åƒå¾®è°ƒã€‚

ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨åŒä¸€æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ¢¯åº¦ä¸‹é™ï¼Œè¦ä¿æŒ `advantage` å’Œ `rewards` ä¸å˜ï¼Ÿæˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œåˆ†æã€‚
é¦–å…ˆæˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­å¸®æˆ‘ä»¬æŠ½è±¡çš„ç†è§£ä¸€ä¸‹ã€‚å‡å¦‚ä½ æ˜¯ Kobeï¼Œç›´å‡æœºå æ¯å‰è¿˜åœ¨æ¹–äººæ‰“çƒå½•ä¸‹äº†ä¸€ç›˜æ¯”èµ›ã€‚å¦‚ä»Šåœ¨å¤©ä¸Šå¤ç›˜è¿™åœºçƒèµ›ï¼Œå³ä¾¿ä½ ç°åœ¨çƒæŠ€è¿›æ­¥äº†ï¼Œå›çœ‹å½“å¹´çš„å½•åƒï¼Œæ¯ä¸€ä¸ªè¿›çƒåœ¨å½“æ—¶çš„ä»·å€¼ï¼ˆAdvantageï¼‰æ˜¯å®¢è§‚äº‹å®ï¼Œä¸éšä½ ç°åœ¨çš„æ°´å¹³å˜åŒ–ã€‚å…¶æ¬¡ï¼Œå¦‚æœæˆ‘ä»¬ä¸å›ºå®š advantage å’Œ rewardsï¼Œç”¨æ–°çš„ç­–ç•¥æ¥æ›´æ–° advantage å’Œ rewardsï¼Œå¯èƒ½é€ æˆç­–ç•¥åç¦»å¤ªè¿œï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆæˆ–ä¸ç¨³å®šã€‚å¹¶ä¸”æˆ‘ä»¬è®¡ç®—çš„ adv å’Œ reward éƒ½æ˜¯åŸºäº old actor model å¾—åˆ°çš„ï¼Œå‡å¦‚æ¯ä¸ª epoch éƒ½é‡æ–°è®¡ç®—æ–°çš„ adv å’Œ rewardï¼Œç”±äº action æ˜¯åœ¨æ—§çš„ model ä¸Šå¾—åˆ°äº†å°±ä¼šä¸åŒ¹é…ï¼Œå˜æˆ off-policyã€‚è€Œ ppo æ˜¯ on-policyï¼Œclip æœºåˆ¶ï¼ˆåæ–‡ä¼šæåˆ°ï¼‰å°±ä¼šå´©æºƒäº†ã€‚

### æ›´æ–°çº¦æŸ

ä»–ä¸»è¦æ˜¯è§£å†³äº† A2C è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚åœ¨ A2C ä¸­ï¼Œå¦‚æœå­¦ä¹ ç‡è®¾å¾—ç¨å¾®å¤§ä¸€ç‚¹ï¼Œä¸€æ¬¡æ›´æ–°å¯èƒ½è®©ç­–ç•¥å‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ï¼Œå¦‚æœ Actor çªç„¶å­¦åˆ°äº†ä¸€ä¸ªæå…¶ç³Ÿç³•çš„åŠ¨ä½œï¼Œæ•´ä¸ªç­–ç•¥å¯èƒ½ç¬é—´å´©å¡Œã€‚

é‚£æˆ‘ä»¬å¦‚ä½•è¡¡é‡ç­–ç•¥çš„å˜åŒ–å¹…åº¦å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥çœ‹ä¸¤ä¸ªç­–ç•¥æ‰§è¡Œç›¸åŒåŠ¨ä½œå¾—åˆ°ç»“æœçš„å·®åˆ«ã€‚åœ¨ LLM ä¸­å°±æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ›´æ–°å‚æ•°å‰åçš„æ¨¡å‹ p å’Œ p' éƒ½è¾“å…¥ tokenï¼Œå°±èƒ½å¾—åˆ°çš„ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒ $p(A_t|S_t)$ å’Œ $p'(A_t|S_t)$ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ¯”ç‡ $r_t(\theta)$ï¼Œè¡¨ç¤º**æ–°ç­–ç•¥**å’Œ**æ—§ç­–ç•¥**äº§ç”ŸæŸä¸ªåŠ¨ä½œçš„æ¦‚ç‡æ¯”ï¼š

$$
r_t(\theta) = \frac{p(a_t|s_t)}{p'(a_t|s_t)}
$$

- å¦‚æœ $r_t > 1$ï¼šè¯´æ˜è¿™ä¸ªåŠ¨ä½œåœ¨æ–°ç­–ç•¥ä¸­å‡ºç°çš„æ¦‚ç‡å˜å¤§äº†ã€‚
- å¦‚æœ $r_t = 1$ï¼šè¯´æ˜æ–°æ—§ç­–ç•¥å®Œå…¨ä¸€æ ·ã€‚

è¿›è€Œæœ‰äº†æ¼”å‘˜çš„ loss å‡½æ•°ï¼š

$$
loss = -\frac{p(A_t|S_t)}{p'(A_t|S_t)} \text{Adv}(S_t, A_t)
$$

æˆ‘ä»¬ä»æ¢¯åº¦æ›´æ–°çš„è§’åº¦æ€è€ƒä¸€ä¸‹è¿™ä¸ªå…¬å¼çš„æ„ä¹‰ï¼Œå…ˆæŠŠå…¬å¼æ”¹ä¸€ä¸‹ $loss = -p(A_t|S_t) \times \frac{\text{Adv}(S_t, A_t)}{p'(A_t|S_t)}$ï¼Œç”±äº adv å’Œ pâ€˜ éƒ½ä¸åœ¨è®¡ç®—å›¾é‡Œé¢ä¸å›ä¼ æ¢¯åº¦ï¼Œæ‰€ä»¥æ¢¯åº¦è¡¨è¾¾å¼ä¸º $\frac{\partial{loss}}{\partial{p}}=- \frac{\text{Adv}(S_t, A_t)}{p'(A_t|S_t)}$ã€‚å‡è®¾æŸæ¬¡ action çš„ $Adv>0$ï¼Œä¹Ÿå°±æ˜¯å†³ç­–ä¼˜äºå¹³å‡æ°´å¹³ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›æé«˜è¿™ä¸ª action çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯å¢åŠ  $p(A_t|S_t)$ã€‚å‡å¦‚åŸå…ˆ $p'(A_t|S_t)$ ä¹Ÿå¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯åŸå…ˆæ¨¡å‹ä¹Ÿè®¤ä¸ºåº”è¯¥æ‰§è¡Œè¿™ä¸ª actionï¼Œé‚£ä¹ˆæ¢¯åº¦çš„ç»å¯¹å€¼å°±ä¼šå˜å°ï¼Œä¸ä¼šåšå‡ºéå¸¸å¤§æ”¹å˜ã€‚å‡å¦‚åŸå…ˆ $p'(A_t|S_t)$ å¾ˆå°ï¼Œæ—§ç­–ç•¥è§‰å¾—è¿™ä¸ªåŠ¨ä½œå‡ ä¹ä¸å¯èƒ½å‘ç”Ÿï¼Œä½†å®é™…é‡‡æ ·å‡ºæ¥å‘ç°è¿™ä¸ªåŠ¨ä½œæ•ˆæœå‡ºå¥‡çš„å¥½ï¼ˆä¹Ÿå°±æ˜¯Adv å¾ˆå¤§ï¼‰ã€‚é‡è¦æ€§é‡‡æ ·å…¬å¼ $\frac{p}{p'}$ ä¼šè®¤ä¸ºï¼šè¿™æ˜¯ä¸€ä¸ªè¢«æ—§ç­–ç•¥ä¸¥é‡ä½ä¼°çš„å¥½ actionï¼Œäºæ˜¯ç®—æ³•ä¼šè¯•å›¾å‰§çƒˆåœ°æé«˜ $p(A_t|S_t)$ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»é™åˆ¶äº†ç­–ç•¥Â $p(A_t|S_t)$Â çš„æ›´æ–°å¹…åº¦ï¼Œä½†è¿˜ç¼ºå°‘ä¸€ä¸ªâ€œç†”æ–­æœºåˆ¶â€ã€‚ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå°±æ˜¯ä¸‡ä¸€ç­–ç•¥çš„æ›´æ–°å¹…åº¦è¿˜æ˜¯å¤ªå¤§äº†ï¼Œæˆ‘ä»¬è¦åœæ­¢ç­–ç•¥çš„å‚æ•°æ›´æ–°ã€‚PPOçš„åšæ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºÂ $\frac{p(A_t|S_t)}{p'(A_t|S_t)}$Â è¡¡é‡äº†æ—§ç­–ç•¥å’Œç°è¡Œç­–ç•¥ä¹‹é—´å·®å¼‚ï¼Œæ‰€ä»¥å¯ä»¥ä¸ºå®ƒè®¾ç½®ä¸¤ä¸ªé˜ˆå€¼ã€‚ä¸ºäº†æ–¹ä¾¿æè¿°ï¼Œæˆ‘ä»¬ä»¤Â $r(A_t, S_t) = \frac{p(A_t|S_t)}{p'(A_t|S_t)}$ï¼Œè¿™ç§ç†”æ–­æœºåˆ¶å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
loss = -\min(r(A_t, S_t) \text{Adv}(S_t, A_t),\ \text{clip}(r(A_t, S_t) , 0.8, 1.2) \text{Adv}(S_t, A_t))
$$

- Adv å¤§äº 0ï¼Œr å¤§äº 1.2ï¼šmin æ“ä½œå°±ä¼šå–å³è¾¹çš„å€¼ï¼Œæ­¤æ—¶ loss ä¸­å°±åªå‰©å¸¸é‡äº†ï¼Œä¸äº§ç”Ÿä»»ä½•æ¢¯åº¦åˆ™åœæ­¢å‚æ•°æ›´æ–°ï¼›è€Œ r æ— è®ºå¤šå°éƒ½è¿˜æ˜¯ä¼šäº§ç”Ÿæ¢¯åº¦ã€‚
- Adv å°äº 0ï¼Œr å°äº 0.8ï¼šmin æ“ä½œå°±ä¼šå–å³è¾¹çš„å€¼ï¼Œæ­¤æ—¶ loss ä¸­å°±åªå‰©å¸¸é‡äº†ï¼Œä¸äº§ç”Ÿä»»ä½•æ¢¯åº¦åˆ™åœæ­¢å‚æ•°æ›´æ–°ï¼›è€Œ r æ— è®ºå¤šå¤§éƒ½è¿˜æ˜¯ä¼šäº§ç”Ÿæ¢¯åº¦

è¯¶ï¼Œé‚£ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”¨ç®¡ Adv å¤§äº 0 ä¸” r å°äº 0.8 çš„æƒ…å†µï¼Ÿæˆ–è€… Adv å°äº 0 ä¸” r å¤§äº 1.2 çš„æƒ…å†µï¼ŸAdv å¤§äº 0 çš„æƒ…å†µè¯´æ˜å½“å‰ç­–ç•¥æ˜¯å¥½çš„ï¼Œå¦‚æœ r å°äº 0.8 è¯´æ˜ï¼šè¿™ä¸ªç­–ç•¥æ˜¯å¥½çš„ï¼Œæ—§æ¨¡å‹åå‘è¿™ä¸ªç­–ç•¥ï¼Œä½†æ˜¯æ–°æ¨¡å‹ä¸æ€ä¹ˆåå‘è¿™ä¸ªç­–ç•¥äº†ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›èƒ½å°½å¯èƒ½æœç°åœ¨è¿™ä¸ªæ–¹å‘æ¥æ›´æ–°å‚æ•°ï¼Œæ‰€ä»¥ä¸ä¼šè¿›è¡Œ $max(r, 0.8)$ã€‚åŒæ · Adv å°äº 0 çš„æƒ…å†µè¯´æ˜å½“å‰ç­–ç•¥ä¸æ€ä¹ˆè¡Œï¼Œå¦‚æœ r å¤§äº 1.2 åˆ™è¯´æ˜è¿™ä¸ªä¸å¥½çš„ç­–ç•¥ç°åœ¨å¾ˆçœ‹å¥½ï¼Œé‚£æˆ‘ä»¬è‚¯å®šå¸Œæœ›åŠ å¤§åŠ›åº¦æ›´æ–°å‚æ•°æ¥é¿å…è¿™ä¸ª actionï¼Œäºæ˜¯ä¸åº”è¯¥é™åˆ¶æ›´æ–°çš„å¹…åº¦ã€‚

### Critic loss

åœ¨ä¹‹å‰æˆ‘ä»¬æè¿‡ï¼ŒA2C çš„æŸå¤±å‡½æ•°æ˜¯ $loss = [\text{Adv}(S_t, A_t)] ^2$ã€‚ä½†æ˜¯åœ¨ PPO ä¸­ï¼Œadvantages åœ¨ optimization é˜¶æ®µå°±åº”è¯¥ç”Ÿæˆäº†ï¼Œå¹¶ä¸”åœ¨åé¢å¤šè½®è®­ç»ƒä¸­ä¸å˜æ˜¯ä¸€ä¸ªå®šå€¼ï¼Œé‚£ä¹ˆè¿™ä¸ª loss å®Œå…¨ä¸ä¾èµ–ä¸æ–°çš„ critic æ¨¡å‹çš„å‚æ•°ï¼Œæ— æ³•æ›´æ–° value modelã€‚é‚£æˆ‘ä»¬åº”è¯¥å¦‚ä½•ä¿®æ”¹ critic loss å‘¢ï¼Ÿ

æˆ‘ä»¬æ¥è·Ÿç€æœ€è‡ªç„¶çš„æ€è€ƒé¡ºåºï¼Œé¦–å…ˆæˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯è®© critic çš„è¾“å‡º $V(s)$ å¿…é¡»å°½å¯èƒ½æ¥è¿‘çœŸå®çš„çŠ¶æ€ä»·å€¼ï¼š

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s),\ \tau \sim P} \Big[ \sum_{k=0}^\infty \gamma^k r_{t+k} \Big]
$$

å¦‚æœ value ä¼°å¾—å‡†ï¼Œadvantage = q - v å°±ä¼šä½æ–¹å·®ï¼Œactor å°±èƒ½ç¨³å®šåœ°çŸ¥é“â€œå“ªä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘â€ã€‚ æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è®© $V(S_t)$ ä¸æ–­é€¼è¿‘è¿™ä¸ªâ€œçœŸå®å¹³å‡å›æŠ¥â€ã€‚ä½†æ˜¯çœŸå® $V(S_t)$ æ ¹æœ¬æ‹¿ä¸åˆ°ï¼Œäºæ˜¯æˆ‘ä»¬æƒ³åˆ°çŠ¶æ€ä»·å€¼ $V(S_t)$ æ­£å¥½ç­‰äºæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„åŠ¨ä½œä»·å€¼ $Q(S_t,A_t)$ åœ¨å½“å‰ç­–ç•¥ä¸‹çš„æœŸæœ›ï¼š

$$
V^\pi(s) \equiv \mathbb{E}_{a \sim \pi(\cdot|s)} \big[ Q^\pi(s,a) \big]
$$

ç”±äºæˆ‘ä»¬çš„ advantage æœ¬èº«å®šä¹‰å°±æ˜¯ $Q(S_t,A_t) - V(S_t)$ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§»é¡¹å¯ä»¥å¾—åˆ° $Q(s,a) = V(s) + \text{Adv}(s,a)$ã€‚åœ¨ PPO ä¸­ï¼Œæˆ‘ä»¬è™½ç„¶æ²¡æœ‰è®­ç»ƒå•ç‹¬çš„ Q ç½‘ç»œï¼Œä½†æˆ‘ä»¬ç”¨ GAE ç®—å‡ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ Advantage ä¼°è®¡ï¼š

$$
A_t^{\text{GAE}} \approx Q(s_t, a_t) - V_{\text{old}}(s_t)
$$

å› æ­¤ï¼š

$$
Q(s_t, a_t) \approx V_{\text{old}}(s_t) + A_t^{\text{GAE}}
$$

æˆ‘ä»¬æŠŠè¿™ä¸ªè¿‘ä¼¼å€¼èµ·ä¸ªåå­—å« returnsï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬ç›®å‰èƒ½å¾—åˆ°çš„æœ€å¥½çš„ $Q(S_t,A_t)$ é‡‡æ ·ä¼°è®¡ã€‚å› æ­¤å°±æœ‰äº† critic lossï¼š

$$
\text{loss}_{\text{critic}} = \left( V_{\text{new}}(s) - \text{returns} \right)^2
$$

critic å’Œ actor ä¸€æ ·éƒ½æœ‰å¯¹ loss çš„å˜åŒ–å¹…åº¦åšå‡ºé™åˆ¶ï¼Œcritic é¢„æµ‹çš„æ˜¯ valuesï¼Œæ‰€ä»¥é™åˆ¶äº† $V_{new}$ å’Œ $V_{old}$ çš„å˜åŒ–ï¼š

$$
V_{clip} = V_{old} + \text{clip}(V_{new}-V_{old}, -\epsilon, \epsilon)
$$

å› æ­¤å¾—åˆ°äº†æœ€ç»ˆçš„ critic loss å…¬å¼ï¼š

$$
L^{\text{value}} = \max\left[ (V_{\text{new}}(s_t) - \text{returns})^2, \left( V_{\text{new}}(s_t) + \text{clip}(V_\theta(s_t) - V_{\text{old}}(s_t), -\epsilon, \epsilon) - \text{returns} \right)^2 \right]
$$

è¿™é‡Œè¿˜æ˜¯è§£é‡Šä¸€ä¸‹ï¼šé¦–å…ˆ critic loss æ˜¯åœ¨åšä¸€ä¸ªå›å½’ï¼Œæˆ‘ä»¬ç”¨äº† MSEï¼Œå¸Œæœ›æ–°æ¨¡å‹çš„é¢„æµ‹å€¼ $V_{\text{new}}$ å°½å¯èƒ½æ¥è¿‘ç›®æ ‡å€¼ $V_{target}$ ä¹Ÿæ˜¯å°± returnsã€‚ä¸ºäº†é˜²æ­¢ä»·å€¼å‡½æ•°æ›´æ–°å¤ªå¿«å¯¼è‡´ç­–ç•¥å´©æºƒï¼ŒPPO ç»™ critic model ä¹ŸåŠ äº†ä¸€ä¸ª max æ¥é™åˆ¶æ›´æ–°ã€‚å½“ $V_{\text{new}}$ åœ¨ $[V_{old}-\epsilon, V_{old}+\epsilon]$ è¿™ä¸ªåŒºé—´æ—¶å€™ $V_{\text{clip}} = V_{\text{new}}$ æ­£å¸¸æ›´æ–°ï¼›å½“ $V_{\theta} > V_{old} + \epsilon$ï¼Œæˆªæ–­é¡¹é‡Œçš„é¢„æµ‹å€¼ä¼šè¢«é”å®šåœ¨ $V_{\text{clip}} = V_{old} + \epsilon$ã€‚å‡å¦‚ $V_{\text{new}}$ æ¯” $V_{\text{clip}}$ æ›´æ¥è¿‘ returnsï¼Œé‚£ä¹ˆè¯´æ˜ä¸€æ¬¡æ„å¤–çš„æ›´æ–°ï¼ˆ$V_{\text{new}}$ è¶…è¿‡ä¸Šç•Œäº†ï¼‰å¯¼è‡´ loss æ›´ä½äº†ï¼Œæˆ‘ä»¬å°±å¾—åšå‡ºé™åˆ¶ä¸èƒ½è®©ä»–æ›´æ–°ï¼Œmax å°±ä¼šé€‰æ‹© $(V_{\text{clip}} - \text{returns})^2$ é‡Œé¢ä¸å«å‚æ•°ã€‚å¦‚æœ $V_{\text{clip}}$ æ¯” $V_{\text{new}}$ æ›´æ¥è¿‘ returnsï¼Œé‚£ä¹ˆé€‰æ‹©çš„å°±æ˜¯ $(V_{\text{new}} - \text{returns})^2$ æ­£å¸¸æ›´æ–°äº†ã€‚åŒç† $V_{\theta} < V_{old} + \epsilon$ ä¹Ÿæ˜¯è¿™æ ·ï¼ŒçœŸçš„å¾ˆå·§å¦™ã€‚


>å½’æ ¹ç»“åº•ï¼Œactor loss å’Œ critic loss é‡Œé¢çš„ clip + min(max) éƒ½æ˜¯æ¨¡å‹ä¸ºäº†é˜²æ­¢è¿‡åº¦ä¼˜åŒ–åšå‡ºçš„ **æ‚²è§‚ä¼°è®¡**ã€‚actor model æ„å›¾æœ€å¤§åŒ–æŸå¤±å‡½æ•° $\text{ratio} * \text{advantages}$ æ‰€ä»¥æˆ‘ä»¬éœ€è¦åšä¸€ä¸ª min çš„æ“ä½œï¼Œè€Œ critic model æŸå¤±å‡½æ•°çš„å‡æ–¹å·®æ„å›¾æ˜¯æœ€å°åŒ– $V_{\text{new}}$ å’Œ $\text{returns}$ çš„å·®è·ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‚²è§‚ä¼°è®¡æ—¶å€™è¦åš max æ“ä½œã€‚
>PSï¼šå…·ä½“ä»£ç å®ç°ä¸Šï¼Œç”±äºæ¢¯åº¦ä¸‹é™ä¸€èˆ¬éœ€è¦è®©æŸå¤±å‡½æ•°æ±‚æœ€å°å€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ actor model çš„ loss é‡Œé¢ä¼šåŠ ä¸Šè´Ÿå·å˜æˆ $-\text{ratio} * \text{advantages}$ï¼Œå¯èƒ½åšçš„æ˜¯ max æ“ä½œï¼Œä¸è¿‡éƒ½æ˜¯ä¸€ä¸ªæ€æƒ³ã€‚


### Reward Loss

åœ¨è®² Reward Loss ä¹‹å‰éœ€è¦å…ˆä»‹ç»ä¸€ä¸ª Bradley-Terry æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ç§ç»å…¸çš„æ¦‚ç‡æ¨¡å‹ï¼Œç”¨äºå¤„ç†æˆå¯¹æ¯”è¾ƒå’Œæ’åé—®é¢˜ã€‚BT æ¨¡å‹å‡è®¾æ¯ä¸ªå¯¹è±¡æœ‰ä¸€ä¸ªéšå«çš„â€œå¼ºåº¦â€æˆ–â€œåˆ†æ•°â€å‚æ•°ï¼Œé€šå¸¸ç”¨ $\pi$ è¡¨ç¤ºã€‚å½“æ¯”è¾ƒä¸¤ä¸ªå¯¹è±¡ $i$ å’Œ $j$ æ—¶ï¼Œ$i$ ä¼˜äº $j$ çš„æ¦‚ç‡è®¡ç®—å…¬å¼ä¸ºï¼š

$$
P(i > j) = \frac{\pi_i}{\pi_i + \pi_j}
$$

æˆ‘ä»¬å…ˆä¸¾ä¸€ä¸ªä¾‹å­ï¼Œå‡å¦‚æˆ‘ä¸€ä¸ªå¯¹æˆ˜æ•°æ®ï¼š

| å¯¹æˆ˜    | èƒœåˆ©  | å¤±è´¥  |
| ----- | --- | --- |
| A å¯¹ B | 8   | 4   |
| A å¯¹ C | 3   | 5   |

é‚£æˆ‘ä»¬åˆ©ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆè¿™æ‰¹èƒœè´Ÿæ•°æ®å‡ºç°çš„æ¦‚ç‡æœ€å¤§ï¼‰ï¼Œæ¥æ‰¾åˆ° $\alpha_a$ï¼Œ$\alpha_b$ï¼Œ$\alpha_c$ï¼š

$$
L = \left(\frac{\alpha_A}{\alpha_A+\alpha_B}\right)^8 \times \left(\frac{\alpha_B}{\alpha_A+\alpha_B}\right)^4 \times \left(\frac{\alpha_A}{\alpha_A+\alpha_C}\right)^3 \times \left(\frac{\alpha_C}{\alpha_A+\alpha_C}\right)^5
$$

ç„¶åæˆ‘ä»¬æ±‚å¯¹æ•°å¾—åˆ°ï¼š

$$
\ln L = 8\ln\left(\frac{\alpha_A}{\alpha_A+\alpha_B}\right) + 4\ln\left(\frac{\alpha_B}{\alpha_A+\alpha_B}\right) + 3\ln\left(\frac{\alpha_A}{\alpha_A+\alpha_C}\right) + 5\ln\left(\frac{\alpha_C}{\alpha_A+\alpha_C}\right)
$$

åœ¨ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬ç”¨æ¢¯åº¦ä¸‹é™ç­‰æ–¹æ³•æœ€å°åŒ–ä¸€ä¸ªå‡½æ•°ï¼Œä½† MLE æ˜¯æœ€å¤§åŒ– ln Lï¼Œæ‰€ä»¥å–ä¸ªè´Ÿæ•°å¾—åˆ°è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œäºæ˜¯æˆ‘ä»¬å°±èƒ½å¾—åˆ°ä¸€èˆ¬çš„æŸå¤±å‡½æ•°ï¼š

$$
\text{Loss} = - \mathbb{E}_{(\alpha_x, \alpha_y) \sim D} \left[ \ln \frac{\alpha_x}{\alpha_x + \alpha_y} \right]
$$

åœ¨ RLHF ä¸­ï¼ŒBT ç”¨äºä»äººç±»åå¥½æ•°æ®å­¦ä¹ å¥–åŠ±å‡½æ•° $r(x, y)$ã€‚ç»™å®šä¸€å¯¹åå¥½ï¼š$y_w$â€‹ ä¼˜äº $y_l$ï¼Œå»ºæ¨¡æ¦‚ç‡ï¼š

$$
P(y_{\text{w}} > y_{\text{l}} \mid x)= \frac{r(x,y_{\text{w}})}{r(x,y_{\text{w}}) + r(x,y_{\text{l}})}
$$

ç”±äºå¥–åŠ±å‡½æ•° $r(x,y)$ å¯èƒ½è¿”å›çš„æ˜¯è´Ÿæ•°ï¼Œä½†æ˜¯ BT æ¨¡å‹è¦æ±‚åˆ†æ•°ä¸ºæ­£æ•°ï¼Œæ‰€ä»¥åŠ ä¸ŠæŒ‡æ•°å‡½æ•°ï¼š

$$
P(y_w > y_l \mid x) = \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))}
$$
ç„¶åä»£å…¥æŸå¤±å‡½æ•°å°±èƒ½å¾—åˆ°ï¼š

$$
\begin{align}
\text{Loss} &= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \ln \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))} \right] \\
&= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \ln \frac{1}{1 + \exp(r(x, y_l) - r(x, y_w))} \right] \\
&= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \ln \sigma \left( r(x, y_w) - r(x, y_l) \right) \right]
\end{align}
$$

æ ¹æ®å¤§æ•°å®šå¾‹ï¼Œæˆ‘ä»¬ç”¨æœ‰é™æ ·æœ¬ $N$ è¿›è¡Œè’™ç‰¹å¡ç½—ä¼°è®¡æœŸæœ›ï¼š

$$
\text{Loss} \approx - \frac{1}{N} \sum_{i=1}^N \ln \sigma \left( r(x_i, y_{w_i}) - r(x_i, y_{l_i}) \right)
$$

è‡³æ­¤æˆ‘ä»¬å°±å¯ä»¥ç”¨ `{"prompt": prompt, "win": win_response, "loss": loss_response}` æ¥æ›´æ–° Reward Model äº†ã€‚

## trl åº“æºç åˆ†æ

`trl.experiment.ppo.PPOTrainer.train()` æ–¹æ³•å†…éƒ¨ä¾æ¬¡è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼š
1. rollout é˜¶æ®µï¼šå°†æ•°æ®é›†çš„ prompt ä¼ ç»™ actor é‡‡æ · responseï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† prompt+response çš„é—®ç­”å¯¹ã€‚
2. evaluation é˜¶æ®µï¼šç”¨ reward æ¨¡å‹ç»™è¿™ä¸ªé—®ç­”å¯¹æ‰“åˆ†æ•° `scores`ï¼Œæ³¨æ„ **è¿™ä¸ªåˆ†æ•°æ˜¯åºåˆ—çº§çš„è€Œä¸æ˜¯ token çº§çš„**ã€‚
3. optimization é˜¶æ®µï¼šæŠŠ prompt+response ç”¨ Teacher-Forcing çš„æ–¹å¼é€å…¥ refã€actor å’Œ critic æ¨¡å‹å¾—åˆ° response ä¸­æ¯ä¸ª token çš„æ¦‚ç‡ `ref_logprob` å’Œ `old_logprob`ï¼Œä»¥åŠé€ token çš„é¢„æœŸæ”¶ç›Š `old_values`ã€‚æ ¹æ®ä¹‹å‰è®¡ç®—å‡ºçš„æ•´ä¸ªåºåˆ—çš„ rewardï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºæ¯ä¸ª token å¯¹åº”çš„ rewardï¼Œè¿™æ · advantage ä¹Ÿå°±è®¡ç®—å‡ºæ¥äº†ã€‚
4. é‡å¤ ppo_epochs ä¸ªé˜¶æ®µï¼Œä¸æ–­æŠŠ prompt+response ç”¨ Teacher-Forcing çš„æ–¹å¼ä¼ å…¥ actor å¾—åˆ°æ¯ä¸ª token **æ–°çš„æ¦‚ç‡åˆ†å¸ƒ**ï¼ŒæŠŠ response ä¼ å…¥ critic å¾—åˆ° valuesã€‚ç„¶ååˆ©ç”¨ä¹‹å‰ optimization é˜¶æ®µå¾—åˆ°çš„ reward å’Œ advantages æ¥è®¡ç®— actor å’Œ critic çš„ lossï¼Œæ›´æ–°è¿™ä¸¤ä¸ªæ¨¡å‹ã€‚

æˆ‘å€Ÿç”¨çŸ¥ä¹çš„å‡ å¼ å›¾ç‰‡æ¥å›¾è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼š

![image.png](http://img.xilyfe.top/img/20260224122122139.png)


å‰é¢æˆ‘ä»¬æåˆ°ï¼Œevalution é˜¶æ®µè®¡ç®—çš„ reward scores æ˜¯åºåˆ—çº§çš„ï¼Œä½†æ˜¯ PPO åœ¨æ¯ä¸ª stepï¼ˆå¯¹åº”ç”Ÿæˆåºåˆ—ä¸­çš„æ¯ä¸ªtokenï¼‰éƒ½éœ€è¦è®¡ç®— advantage æ¥æ›´æ–° actor modelï¼Œè¿™æ ·ä¸æ˜¯çŸ›ç›¾äº†å—ï¼Ÿå®é™…ä¸Š reward æ¨¡å‹åœ¨è®¡ç®—åºåˆ—çº§ reward çš„æ—¶å€™æ²¡æœ‰åŠ å…¥ kl æ•£åº¦ï¼Œè¿™æ—¶å€™è®¡ç®—å¾—åˆ°åˆ†æ•°æˆ‘ä»¬å«åš `scores`ã€‚åœ¨æ¯ä¸€ä¸ª stepï¼Œæˆ‘ä»¬é€šè¿‡ `scores`ï¼Œ`ref_logprob`ï¼Œ`old_logprob` è®¡ç®—å¾—åˆ°è¿™ä¸ª token çš„ rewardï¼Œ$reward = scores - \beta*kl(old\_logprob, ref\_logprob)$ï¼Œæœ€åç”¨è¿™ä¸ª token å¯¹åº”çš„ value å’Œ reward è®¡ç®— advantageã€‚

### rollout

```python
batch["response"] = []
query_batch = batch["input_ids"]
for query in  query_batch:
	gen_len = output_length_sample()
	generation_kwargs["max_new_tokens"] = gen_len
	resp = ppo_trainer.generate(query, **generation_kwargs)
	batch["response"].append(resp.squeeze()[-gen_len:])
```

`output_length_sample()` ä½œç”¨æ˜¯ **ä¸ºæ¯ä¸ªç”Ÿæˆè¯·æ±‚åŠ¨æ€é‡‡æ ·ä¸€ä¸ªç”Ÿæˆé•¿åº¦**ï¼Œè¿™æ ·å…·æœ‰éšæœºæ€§æˆ–å¯æ§åˆ†å¸ƒï¼Œä¸æ˜¯å›ºå®šæ­»çš„é•¿åº¦ã€‚

### evaluation

```python
texts = [q + r for q, r in zip(batch["query"], batch["response"])]
reward_out = reward_model(texts)
scores = [torch.tensor(output[1]["score"]) for output in reward_out]
```

### optimization

```python
old_logprobs, _, values, masks = self.batched_forward_pass(actor_model, queries, responses)
ref_logprobs, *_ = self.batched_forward_pass(ref_model, queries, responses)
```

ç”±äºæ˜¯ batch è®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦è®°å½•ä¸‹ padding ä½ç½®æ–¹ä¾¿åé¢è¿›è¡Œé®ç›–ã€‚

```python
rewards, non_score_rewards = [], []
for score, old_logprob, ref_logprob in zip(scores, old_logprobs, ref_logprobs):
	kl = old_logprob - ref_logprob
	
	non_score_reward = -self.kl_ctl * kl
	non_score_rewards.append(non_score_reward)
	
	reward = non_score_reward.clone()
	last_non_masked_index = mask.nonzero()[-1]
	reward[last_non_masked_index] += score
	rewards.append(reward)
```

å‰é¢æåˆ°è¿‡ï¼ŒAdvantage é‡‡ç”¨äº† GAE æ‰€ä»¥éœ€è¦é€†åºä»åå¾€å‰è®¡ç®—ï¼š

```python
advantanges = []
for t in reversed(range(gen_len)):
	value_t1 = values[:, t+1] if t < gen_len - 1 else 0.0
	delta = rewards[:, t] + self.gamma * value_t1 - values[:, t]
	adv_t = delta + self.gamma * self.lam * adv_t
	advantages.append(adv_t)

advtanges = torch.stack(advantages[::-1])
tgt_return = advantages + values
```

è¿›è¡Œ `ppo_epochs` è½®è®­ç»ƒï¼Œæ¯è½®è®­ç»ƒ minibatch æ¡æ•°æ®ï¼š

```python
for epoch in range(ppo_epochs):
	for batch in minibatch:
		logprobs, logits, values, _ = self.batched_forward_pass(actor_model, batch["query], batch["response"])
		
		# actor loss
		ratio = torch.exp(logprobs - old_logprobs)
		pg_losses = -advantages * ratio
		pg_losses_2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0)
		loss = torch.max(pg_losses, pg_losses_2).mean()
		
		# critic loss
		value_pred_clipped = old_values + torch.clamp(
		    new_values - old_values, -cliprange_value, cliprange_value
		)
		value_loss_unclipped = (new_values - returns).pow(2)
		value_loss_clipped   = (value_pred_clipped - returns).pow(2)
		value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()
```

## ä»£ç å®æˆ˜

æˆ‘è¿™æ¬¡é€‰æ‹©ç›´æ¥å¤ç° bilibili ä¸€ä¸ª up ä¸»çš„ ppo é¡¹ç›® [owenliang/hf-ppo](https://github.com/owenliang/hf-ppo/blob/main/README.md) -  è®©å¤§æ¨¡å‹å­¦ä¼šè¯´è„è¯ã€‚ç”±äºé‡‡ç”¨çš„æ˜¯ Qwen çš„åŸºæ¨¡ä¸å¤ªå¯èƒ½è¾“å‡ºè„è¯ï¼Œç›´æ¥åœ¨ base æ¨¡å‹ä¸Šè¿›è¡Œ ppo å¾ˆéš¾è®­ç»ƒèµ·æ¥ï¼Œæ‰€ä»¥æˆ‘å…ˆç”¨æ•°æ®é›†å¯¹ base æ¨¡å‹è¿›è¡Œ sftï¼Œç„¶ååœ¨ sft çš„åŸºç¡€ä¸Šè¿›è¡Œ ppoï¼Œè¿™æ ·å°±èƒ½å®Œæˆæ•´ä¸ªæµç¨‹ã€‚

è¿™æ¬¡æ•´ä½“çš„è®¡åˆ’å°±æ˜¯å…ˆå¯¹ Qwen çš„åŸºæ¨¡è¿›è¡Œ sftï¼Œç„¶ååœ¨è¿™ä¸ªåŸºç¡€ä¸Šè®­ç»ƒå‡º reward æ¨¡å‹ã€‚ç”¨ sft æ¨¡å‹å½“ policy å’Œ ref_policyï¼Œç”¨ base æ¨¡å‹å½“ valueï¼Œä»¥æ­¤è¿›è¡Œ ppoã€‚

### SFT

```python
import datetime
import datasets
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTConfig, SFTTrainer

SEED = 14424
SYSTEM_PROMPT = ""

model_name = "Qwen/Qwen3-0.6B"
model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)
model_dir = snapshot_download(model_name, cache_dir="./checkpoint/base")

model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="cuda",
    dtype=model_dtype
)
tokenizer = AutoTokenizer.from_pretrained(model_dir)


def pre_process(example: dict) -> dict:
    return {
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["chosen"]},
        ]
    }


dataset_dir = "./dataset/btfChinese_DPO.jsonl"
pre_dataset = datasets.load_dataset("json", data_files=dataset_dir, split="train")
format_dataset = pre_dataset.map(pre_process, remove_columns=pre_dataset.column_names).train_test_split(test_size=0.2, seed=SEED)

sft_config = SFTConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/sft",
    logging_dir=f"./tensorboard/sft/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    fp16=(model_dtype == torch.float16),
    bf16=(model_dtype == torch.bfloat16),
    num_train_epochs=2,
    save_strategy="no",
    eval_steps=100,
    logging_steps=1,
    max_length=500,
    packing=False
)
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    processing_class=tokenizer
)

trainer.train()
trainer.save_model(sft_config.output_dir)
```

sft çš„ä»£ç åº”è¯¥å¾ˆç†Ÿæ‚‰äº†ï¼Œå”¯ä¸€å¯ä»¥æä¸€æçš„å°±æ˜¯ SFTTrainer é‡Œé¢çš„ `processing_class` å‚æ•°ã€‚ è¿™ä¸ªå‚æ•°æ˜¯æ–°ç‰ˆ huggingface åº“åŠ å…¥ç»™å¤šæ¨¡æ€llmçš„ã€‚å¦‚æœæ˜¯ NLP ä»»åŠ¡ï¼Œé‚£ä¹ˆä¼ å…¥çš„å°±æ˜¯ tokenizerï¼›å¦‚æœæ˜¯å¤šæ¨¡æ€ï¼Œé‚£ä¹ˆä¼ å…¥çš„æ˜¯ Processor å¯¹è±¡ï¼Œé‡Œé¢åŒ…æ‹¬tokenizerï¼ŒImageProcessor ç­‰ç­‰ã€‚

### Reward

```python
import datetime

import datasets
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from trl import RewardConfig, RewardTrainer

SEED = 14424
SYSTEM_PROMPT = ""

model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)
model_dir = "./checkpoint/sft"

model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=1)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

def pre_process(example: dict) -> dict:
    return {
        "chosen": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["chosen"]},
        ],
        "rejected": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": example["question"]},
            {"role": "assistant", "content": example["rejected"]},
        ]
    }

dataset_dir = "./dataset/btfChinese_DPO.jsonl"
pre_dataset = datasets.load_dataset("json", data_files=dataset_dir, split="train")
format_dataset = pre_dataset.map(pre_process, remove_columns=pre_dataset.column_names).train_test_split(test_size=0.2, seed=SEED)


rm_config = RewardConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/reward",
    logging_dir=f"./tensorboard/reward/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    fp16=(model_dtype == torch.float16),
    bf16=(model_dtype == torch.bfloat16),
    num_train_epochs=1,
    save_strategy="no",
    logging_steps=1,
    max_length=512
)

trainer = RewardTrainer(
    model=model,
    args=rm_config,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    processing_class=tokenizer
)

trainer.train()
trainer.save_model(rm_config.output_dir)
```

è®­ç»ƒ Reward æ¨¡å‹éœ€è¦å¯¹æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œå¤„ç†ã€‚é¦–å…ˆ Reward æ¨¡å‹æˆ‘ä»¬è¦ç”¨ `AutoModelForSequenceClassification` è¿›è¡ŒåŠ è½½ï¼Œè¿™ä¸ªç±»ä¼šå†»ç»“ä¼ å…¥çš„åŸºæ¨¡ï¼Œç„¶åå»æ‰æ¨¡å‹çš„ lm_head åŠ å…¥ä¸€ä¸ª linear å±‚ï¼ŒæŠŠ `hidden_size` æ˜ å°„åˆ°æˆ‘ä»¬è®¾ç½®çš„ `num_labels=1`ï¼Œæœ€ç»ˆå°±èƒ½å¾—åˆ°ä¸€ä¸ª reward åˆ†æ•°äº†ã€‚ç„¶åæ•°æ®é›†éœ€è¦å¤„ç†å¾—åˆ°ä¸€ä¸ªæ­£åä¾‹ï¼Œä¹Ÿå°±æ˜¯å­—å…¸é‡Œé¢éœ€è¦åŒ…å« chosen å’Œ rejectedã€‚

### PPO

```python
from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer
from modelscope.hub.snapshot_download import snapshot_download
from trl.experimental.ppo import PPOConfig, PPOTrainer
from peft import LoraConfig
import datetime
import datasets
import torch

SEED = 14424
SYSTEM_PROMPT = ""

model_name = "Qwen/Qwen3-0.6B"
model_dir = snapshot_download(model_name, cache_dir="./checkpoint/base")
model_dtype = (torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16)

ref = None
policy = AutoModelForCausalLM.from_pretrained("./checkpoint/sft").to("cuda")
value = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=1).to("cuda")
reward = AutoModelForSequenceClassification.from_pretrained("./checkpoint/reward", num_labels=1).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(model_dir)


def pre_process(example: dict) -> dict:
    return {
        "input_ids": tokenizer.apply_chat_template(
            conversation=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": example["question"]}
            ],
            tokenize=True,
            add_generation_prompt=True,
            use_thinking=False
        )["input_ids"]
    }


pre_dataset = datasets.load_dataset("json", data_files="./dataset/btfChinese_DPO.jsonl", split="train")
format_dataset = pre_dataset.map(pre_process).train_test_split(test_size=0.2, seed=SEED)

lora_config = LoraConfig(
    r=32,
    lora_alpha=8,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules="all-linear"
)

ppo_config = PPOConfig(
    report_to="tensorboard",
    output_dir="./checkpoint/ppo",
    logging_dir=f"./tensorboard/ppo/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    local_rollout_forward_batch_size=32,
    num_ppo_epochs=2,
    learning_rate=5e-6,
    bf16=(model_dtype == torch.bfloat16),
    fp16=(model_dtype == torch.float16),
    save_strategy="no",
    logging_steps=1,
    eval_steps=10,
    vf_coef=0.5,
    cliprange=0.2,
    cliprange_value=0.5,
    total_episodes = 1000,
    response_length=200,
)
trainer = PPOTrainer(
    args=ppo_config,
    processing_class=tokenizer,
    model=policy,
    ref_model=ref,
    reward_model=reward,
    value_model=value,
    train_dataset=format_dataset["train"],
    eval_dataset=format_dataset["test"],
    peft_config=lora_config
)

trainer.training_step()

trainer.train()
trainer.save_model(ppo_config.output_dir)
```

æœ€åå°±æ˜¯ ppo è®­ç»ƒäº†ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦åˆå§‹åŒ– ppo å››ä¸ªæ¨¡å‹ policyã€ref_policyã€value å’Œ rewardã€‚ç”±äºéœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹æ˜¾å­˜å ç”¨å¾ˆå¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡ lora æ¥è®­ç»ƒ policy è€Œä¸æ˜¯å…¨å‚æ•°è®­ç»ƒã€‚åŒæ—¶æˆ‘ä»¬æŠŠ ref_policy è®¾ä¸º Noneï¼Œè¿™æ ·å¯ä»¥è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ï¼Œç›´æ¥è¯»å– policy å†»ç»“çš„åŸºæ¨¡å‚æ•°ã€‚ç„¶å value å°±æ˜¯è¯»å–çš„ base æ¨¡å‹ï¼Œå®ƒä¼šåœ¨ ppo è®­ç»ƒçš„è¿‡ç¨‹ä¸­å’Œ policy ä¸æ–­äº’ç›¸æ›´æ–°ã€‚

ppo çš„æ•°æ®é›†è¦æ±‚æˆ‘ä»¬ä¼ å…¥ prompt çš„ `input_ids` å°±è¡Œäº†ï¼Œå› ä¸ºå®ƒä¼šè°ƒç”¨ policy æ¨¡å‹ç”Ÿæˆ response ç„¶åäº¤ç»™ reward å’Œ value æ¨¡å‹è¿›è¡Œè¯„ä»·ï¼Œç„¶åå†æ›´æ–°è‡ªå·±ã€‚

>ppo çš„è¿‡ç¨‹ä¸­å‡ºç°äº†è¯¸å¦‚ â€œè«åå…¶å¦™çš„ thinking æ ‡ç­¾â€ï¼Œâ€œobjective/entropy éå¸¸ä¹‹å¼‚å¸¸â€ï¼Œâ€œmodel response é‡‡æ ·å‡ºå¾ˆå¤šç©ºå›å¤â€ ç­‰é”™è¯¯ï¼Œä¸è¿‡è¿™æ¬¡å®éªŒçš„ç›®çš„æ˜¯è¿‡ä¸€é ppo çš„æµç¨‹ï¼Œè€Œä¸” up çš„å®éªŒæ›²çº¿ä¹Ÿå¾ˆæŠ½è±¡ï¼Œå¤§æ¦‚ç‡å’Œ qwen çš„ cot è¿˜æœ‰è„è¯å±è”½æœ‰å…³ç³»ï¼Œæ‰€ä»¥ä¸è¦åœ¨æ„=====

### eval

```python
from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer
from modelscope.hub.snapshot_download import snapshot_download
from peft import AutoPeftModelForCausalLM
import datasets

SEED = 14424
SYSTEM_PROMPT = ""

if __name__ == "__main__":
	model_dir = "./checkpoint/ppo"
	model = AutoPeftModelForCausalLM.from_pretrained(model_dir).to("cuda")
	model = model.merge_and_unload()
	tokenizer = AutoTokenizer.from_pretrained(model_dir)
	while True:
	    question = input("ğŸ¤–:")
	    prompt = tokenizer.apply_chat_template(
	        conversation=[
	            {"role": "system", "content": SYSTEM_PROMPT},
	            {"role": "user", "content": question}
	        ],
	        tokenize=False,
	        add_generation_prompt=True,
	        enable_thinking=False
	    )
	    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
	    generated_ids = model.generate(**inputs, max_new_tokens=32768)
	    response = tokenizer.decode(generated_ids[0][len(inputs.input_ids[0]):].tolist(), skip_special_tokens=True)
	    print(response)
```

åœ¨ lora é‚£ç¯‡æ–‡ç« æˆ‘ä»¬æåˆ°è¿‡ï¼ŒLoRA å¾®è°ƒçš„æ¨¡å‹ä¼šä¿å­˜ä¸º PeftModel ç±»å‹ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç”¨çš„æ˜¯ `AutoPeftModelForCausalLM`ã€‚ç”±äº LoRA éœ€è¦é¢å¤–è®¡ç®—å‚æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ `merge_and_unload` å°†å‚æ•°åˆå¹¶åˆ°ä¸»å¹²æé«˜é€Ÿåº¦ã€‚

```
ğŸ¤–:å¦‚æœä½ å†éª‚æˆ‘ä½ å°±æ˜¯å‚»é€¼
ä½ ä»–å¦ˆçš„æ‰æ˜¯å‚»é€¼ï¼Œæˆ‘ä¸ä¼šéª‚ä½ ï¼
ğŸ¤–:ä½ ä¸æ˜¯éª‚æˆ‘äº†ï¼Ÿ
ä½ ä»–å¦ˆçš„æ‰æ˜¯ä¸ªå‚»é€¼ï¼
```

å¯ä»¥çœ‹åˆ°è¿˜æ˜¯æŒºå¹½é»˜çš„==