---
title: "CS224N Lecture 1: Intro and Word Vectors"
date: 2025-11-05T11:24:11+08:00
authors:
  - Xilyfe
series:
  - CS224N
tags:
  - 深度学习
lastmod: 2026-01-30T02:49:23+08:00
featuredImage: http://img.xilyfe.top/img/20260130144509787.png
---

![](http://img.xilyfe.top/img/20260130144846647.png)


## 1. 独热编码

每个不同的向量代表一个单词，例如: 
``` 
motel = [0 0 0 0 0 0 1]  
hotel = [0 0 0 0 0 1 0]
```
这个方案的缺点很明显: 没法判断每个向量之间的相似性，虽然上面motel和hotel代表的向量是正交的，但它们之间相互有关联；其次这些one-bot vector的维度会非常大，因为词汇表中往往有几十万的单词。

## 2. 词向量

基于单词的上下文获得它的含义，通过学习各个单词的含义得到更加密集的词向量。

```
expect = [
          0.286,
          0.792
          ...
         ]
```

## 3. Word2vec

重要假设：文本中里的最近的词语相似度越高

![](http://img.xilyfe.top/img/20260130144914155.png)

- CBOW: 用中心词预测上下文词
- skip-gram: 用上下文词预测中心词

> 上下文词的数量取决于窗口大小，中心词是每次遍历的词语。实战中除了用上下文来预测中心词，还需要用到负样本词，也就是非上下文词，具体用多少或者是否采用，取决于语料库大小。

对于每一个中心词，我们给其他词出现的概率定义为：
$$ 
P(w_{other} \mid w_{center}) 
$$
假设j是窗口大小，对于每个中心词，它上下文词语出现的概率即为：
$$
P(w_{center+j} \mid w_{center})
$$
我们希望P(上下文词∣中心词) 越高,这样就说明我们预测越准确。

对于单个中心词的似然函数是：
$$
L_t = \prod_{-c \le j \le c, j \ne 0} P(w_{t+j} \mid w_t)
$$

> 假设玩一个预测游戏,中心词是 “猫”，窗口内上下文词有两个：["在", "睡觉"]。模型预测：“在”出现的概率 = 0.8，“睡觉”出现的概率 = 0.9，同时猜对两个词的概率就是：0.8×0.9=0.72，乘起来，就是“两个词都预测对的概率”。

整个语料库的似然函数/目标损失函数就是：
$$
L = \prod_{t=1}^{T} \prod_{-c \le j \le c, j \ne 0} P(w_{t+j} \mid w_t)
$$

一般使用两个技巧来优化 Loss:  
1. 一般都是最小化而不是最大化，所以对Loss取负数，之后采用梯度下降
2. 采用取对数减小数字大小，方便计算

最后的目标损失函数即为：
$$
J = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \le j \le c, j \ne 0} \log P(w_{t+j} \mid w_t)
$$

给定中心词预测上下文词的公式如下：
$$
P(w_o \mid w_i) = \frac{\exp(v_{w_o}'^T v_{w_i})}{\sum_{w=1}^{V} \exp(v_w'^T v_{w_i})}
$$

1. 通过中心词和上下文词的内积，计算他们之间的相似度
2. 通过exp取指数，它保证输出是正数。概率需要是正数
3. 通过归一化使得它是0-1的小数，也就是说
$$
P(w_o \mid w_i) = \frac{\text{该词在上下文i中的得分}}{ \text{所有词在上下文i中的得分}}
$$


那前面说的中心词和上下文词的向量是如何得到的呢？一开始模型会为这些词随机一个向量，然后在梯度下降的过程中优化这些向量。

---
Word2Vec的目标函数/损失函数与深度学习中回归问题的损失函数有所不同。
- 回归问题通常是用对已知值和预测值的差进行拟合
- Word2Vec只是要求模型“预测的概率分布”尽量让真实上下文词的概率高


Word2Vec 的思想是“最大化概率”，它不直接比较预测值和真实值，而是要求模型“预测的概率分布”尽量让真实上下文词的概率高。

即：
$$
max\prod{P(真实上下文词∣中心词)}
$$

回归问题是希望"预测值-真实值"尽可能小，而Word2Vec是希望概率尽可能大（取负数之后尽可能小），也可以看做是一个Loss Function。