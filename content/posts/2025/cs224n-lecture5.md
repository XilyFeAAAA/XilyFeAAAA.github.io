---
title: "Lecture 5: Recurrent Neural Networks"
date: '2025-11-22T11:24:11+08:00'
authors: [Xilyfe]
series: ["CS224N"]
tags: ["深度学习"]
--- 

## 语言模型

1. LM可以通过一句话的前n个词，计算出下一个词是某个词的概率。

$$
P(x^{t+1}|x^{t},...,x^1)
$$

2. LM可以计算出某句话出现的概率

$$
P(x^1,...,x^{t})=P(x^1) \cdot P(x^2|x^1) \cdot ...
$$

## N-gram

N-gram 指的是一堆连续的词，而 N 指的是这一堆词的个数。一个 N-gram 的 LM 就可以根据前 N-1 个词，预测出第 N 个词的概率。

> 对于一个N-gram的LM，我们需要做一个假设：某个词出现的概率只由其前N-1个词决定。

假如我们有一个 3-gram 的 LM，要根据“今天天气怎么样”来预测下一个词，那么只能使用“怎么样”来预测，而不看之前的字。

$$
P(x^t|x^{t-1},...,t^1) \\
=P(x^t|x^{t-1},...,x^{x-N+1}) \\
=\frac{x^t,x^{t-1},...,t^{t-N+1}}{P(x^{t-1},...,x^{t-N+1})}
$$

- 等式1就是根据之前的假设得到的
- 等式2是通过条件概率计算得到的
- 概率用语料库中出现的频率来估计

**稀疏性问题**

1. 当分子在语料中不存在的时候，可以采用一个很小的数补上，例如0.25
2. 当分母在语料中不存在的时候，我们可以采用 backoff 的策略，统计(N-2)-gram的个数。

**存储问题**

虽然这种基于计数的LM的很简单，但是我们必须穷举出预料中所有可能的N-gram，并逐一去计数、保存，N一旦大起来的话，模型的大小就会陡增。

---

对于文本生成来说，这种基于计数的LM则有很大问题了。这里拿CS224N的PPT上的一个例子来说明：

![](http://img.xilyfe.top/img/20260119120456350.png)



## 循环神经网络 RNN

![](http://img.xilyfe.top/img/20260119120511790.png)

RNN 的设计思路如下：

$$
h_t=tanh(Wx \cdot x_t + W_h \cdot h_{t-1} + b_h)
$$


1. 传入文本的 one-hot 编码
2. 在嵌入层中取出每个词对应的 embedding
3. 开始循环处理：首先初始化一个矩阵 $w_0$，每次循环用一个矩阵$w_i$和词向量相乘，再与上一个矩阵相加得到新矩阵，循环往复。
4. 最后进行 softmax

但是 RNN 的缺点也很明显：

1. 循环的次数和文本长度有关，效率低
2. 长文本难以捕捉相互关系

---

**训练过程**


我们把序列作为输入，输入到RNN网络中，每一步都可以得到一个输出，这个输出即为当前步的下一个词的概率分布。我们使用这个概率分布可以和真实的概率分布计算一个损失。明确了损失函数，我们就很容易去训练了。

![](http://img.xilyfe.top/img/20260119120523409.png)

以上图为例，首先输入the，得到下一个词的概率分布，与students计算损失。之后输入第二字词students与正确的opened计算损失。最后将损失相加计算平均损失。

> 在大训练文本上通常会固定大小切割一次，不会考虑语言细节。并且这个好处在于可以组合成为矩阵方便输入。