<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Module on 多辣加香菜</title>
    <link>http://xilyfeAAAA.github.io/tags/module/</link>
    <description>Recent content in Module on 多辣加香菜</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 14 Feb 2026 04:47:45 +0800</lastBuildDate>
    <atom:link href="http://xilyfeAAAA.github.io/tags/module/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bilateral LSTM</title>
      <link>http://xilyfeAAAA.github.io/posts/bilstm/</link>
      <pubDate>Thu, 27 Nov 2025 11:49:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/bilstm/</guid>
      <description>&lt;h2 id=&#34;双向-lstm&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%8f%8c%e5%90%91-lstm&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;双向 LSTM&lt;/h2&gt;&lt;p&gt;有些时候预测可能需要由前面若干输入和后面若干输入共同决定，这样会更加准确。因此提出了双向循环神经网络，网络结构如下图。可以看到 Forward 层和 Backward 层共同连接着输出层，其中包含了 6 个共享权值 w1-w6。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN</title>
      <link>http://xilyfeAAAA.github.io/posts/rnn/</link>
      <pubDate>Wed, 19 Nov 2025 18:06:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/rnn/</guid>
      <description>&lt;blockquote&gt;&#xA;  &lt;p&gt;在 CS224N 的课程中学习了 RNN 的基本知识，为了深入了解背后的机制和代码实现，我让 GPT 设计了一个 RNN 相关的深度学习任务，通过 PyTorch 手搓一个 RNN 网络。&lt;/p&gt;&#xA;&#xA;&lt;/blockquote&gt;&lt;h2 id=&#34;实验题目&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%ae%9e%e9%aa%8c%e9%a2%98%e7%9b%ae&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;实验题目&lt;/h2&gt;&lt;p&gt;使用 RNN 实现英文字母序列预测任务（Character-Level Sequence Prediction）&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
