<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on 多辣加香菜</title>
    <link>http://xilyfeAAAA.github.io/tags/transformer/</link>
    <description>Recent content in Transformer on 多辣加香菜</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 14 Feb 2026 04:47:48 +0800</lastBuildDate>
    <atom:link href="http://xilyfeAAAA.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mask On Transformer</title>
      <link>http://xilyfeAAAA.github.io/posts/tips_about_mask/</link>
      <pubDate>Sat, 24 Jan 2026 12:22:15 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/tips_about_mask/</guid>
      <description>&lt;p&gt;在 Transformer 中同时运用了两种掩码技术：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;用于&lt;strong&gt;处理非定长序列&lt;/strong&gt;的 padding mask&lt;/li&gt;&#xA;&lt;li&gt;用于&lt;strong&gt;防止标签泄露&lt;/strong&gt;的 causal mask&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;padding-mask&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#padding-mask&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;Padding Mask&lt;/h2&gt;&lt;p&gt;NLP 任务中，输入的长度往往不是统一的，训练的数据集里面样本长度各有不同。但是我们在实际训练中，往往需要把多个数据合成一个大的 batch 一同训练，这样可以充分利用显卡的性能。那么问题就来了，不同长度的文本如何合成一个大 batch 呢。NLP 的解决思路是：把所有输入的文本统一成一个固定长度，多余的位置用特殊字符 &amp;lt;PAD&amp;gt; 来填充。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KVCache</title>
      <link>http://xilyfeAAAA.github.io/posts/kvcache/</link>
      <pubDate>Wed, 03 Dec 2025 16:25:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/kvcache/</guid>
      <description>&lt;div class=&#34;details admonition tip open&#34;&gt;&#xA;    &lt;div class=&#34;details-summary admonition-title&#34;&gt;&#xA;        &lt;span class=&#34;icon&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 352 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M96.06 454.35c.01 6.29 1.87 12.45 5.36 17.69l17.09 25.69a31.99 31.99 0 0 0 26.64 14.28h61.71a31.99 31.99 0 0 0 26.64-14.28l17.09-25.69a31.989 31.989 0 0 0 5.36-17.69l.04-38.35H96.01l.05 38.35zM0 176c0 44.37 16.45 84.85 43.56 115.78 16.52 18.85 42.36 58.23 52.21 91.45.04.26.07.52.11.78h160.24c.04-.26.07-.51.11-.78 9.85-33.22 35.69-72.6 52.21-91.45C335.55 260.85 352 220.37 352 176 352 78.61 272.91-.3 175.45 0 73.44.31 0 82.97 0 176zm176-80c-44.11 0-80 35.89-80 80 0 8.84-7.16 16-16 16s-16-7.16-16-16c0-61.76 50.24-112 112-112 8.84 0 16 7.16 16 16s-7.16 16-16 16z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;KVCache 的应用场景&lt;span class=&#34;details-icon&#34;&gt;&lt;svg class=&#34;icon&#34;&#xA;    xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 256 512&#34;&gt;&lt;!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --&gt;&lt;path d=&#34;M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z&#34;/&gt;&lt;/svg&gt;&lt;/span&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;details-content&#34;&gt;&#xA;        &lt;div class=&#34;admonition-content&#34;&gt;KV Cache 的主要目的是 “加速自回归生成”，在生成下一个 token 时，只需要计算当前 token 的 query、key、value。之前的 key/value 被缓存，直接拼接使用，避免重复计算整个历史序列的 attention，这在推理阶段能显著提升速度。但是在预训练阶段，例如 LLaMa 或者 GPT 采用的都是 Teacher-Forcing 这种并行训练，我们把 logits 和 labels 错位。这种计算是并行的，不需要“增量”生成，所以使用 KV Cache 反而会增加代码复杂度和内存开销，收益很小甚至为负。&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&#xA;&lt;h2 id=&#34;前情提要&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e5%89%8d%e6%83%85%e6%8f%90%e8%a6%81&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Teacher-Forcing vs 自回归生成&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RoPE</title>
      <link>http://xilyfeAAAA.github.io/posts/rope/</link>
      <pubDate>Fri, 28 Nov 2025 16:23:11 +0800</pubDate>
      <guid>http://xilyfeAAAA.github.io/posts/rope/</guid>
      <description>&lt;h2 id=&#34;作用&#34; class=&#34;headerLink&#34;&gt;&#xA;    &lt;a href=&#34;#%e4%bd%9c%e7%94%a8&#34; class=&#34;header-mark&#34;&gt;&lt;/a&gt;作用&lt;/h2&gt;&lt;p&gt;RoPE 相对于正余弦位置编码和可学习位置编码，更能够表达&lt;strong&gt;相对位置信息&lt;/strong&gt;，便于模型捕捉序列中元素之间的关系，还便于模型泛化到更长的序列，支持超长文本推理。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
